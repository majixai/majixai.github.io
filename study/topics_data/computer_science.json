[
    {
        "title": "CS: Array vs. Linked List",
        "content": "<h3>Concept: Linear Data Structures</h3><p>Two fundamental ways to store a sequence of elements.</p><h3>Array:</h3><ul><li>Stores elements in a <strong>contiguous block of memory</strong>.</li><li><strong>Access:</strong> O(1) - Direct access by index (<code>arr[i]</code>) is very fast.</li><li><strong>Insertion/Deletion (Middle):</strong> O(n) - Slow, requires shifting subsequent elements.</li><li><strong>Memory:</strong> Fixed size (often) or requires reallocation if dynamic.</li></ul><h3>Linked List:</h3><ul><li>Stores elements as <strong>nodes</strong>, each containing data and a pointer/reference to the next node. Nodes can be scattered in memory.</li><li><strong>Access:</strong> O(n) - Must traverse the list from the head to reach an element by position.</li><li><strong>Insertion/Deletion (Known Position/Node):</strong> O(1) - Fast, only requires updating pointers of adjacent nodes.</li><li><strong>Memory:</strong> Dynamic size, memory overhead per node (for the pointer).</li></ul><h3>Takeaway:</h3><p>Choose arrays for fast random access. Choose linked lists for frequent insertions/deletions in the middle of the sequence.</p>"
    },
    {
        "title": "CS: Hash Table Collisions",
        "content": "<h3>Concept: Handling Key Conflicts</h3><p>What happens when different keys hash to the same index in a hash table?</p><h3>Ideal Hash Table:</h3><ul><li>Average O(1) for Insert, Search, Delete.</li><li>Relies on a good hash function distributing keys evenly.</li></ul><h3>Collision:</h3><ul><li>Occurs when <code>hash(key1) == hash(key2)</code> for <code>key1 != key2</code>.</li><li>Must have a strategy to store both elements.</li></ul><h3>Common Strategies:</h3><ul><li><strong>Chaining:</strong> Each array slot holds a pointer to a linked list (or other structure) of all elements that hash to that index.<ul><li><strong>Pros:</strong> Simple to implement, deletion is easy.</li><li><strong>Cons:</strong> Requires extra space for pointers, cache performance can be poor traversing lists.</li></ul></li><li><strong>Open Addressing:</strong> If a slot is occupied, probe for the next available slot using a predefined sequence (linear, quadratic probing, double hashing). Element is stored *directly* in the table.<ul><li><strong>Pros:</strong> Better cache performance, no extra space for pointers per element.</li><li><strong>Cons:</strong> Deletion is complex (may break search chains), can suffer from clustering (especially linear probing).</li></ul></li></ul><h3>Takeaway:</h3><p>Collision resolution is key to Hash Table performance. Chaining is generally simpler, while Open Addressing can offer better cache locality but requires more complex management, especially for deletions.</p>"
    },
    {
        "title": "CS: BST vs. Balanced BST",
        "content": "<h3>Concept: Tree Search Performance</h3><p>Organizing data hierarchically for efficient lookups, and guaranteeing performance.</p><h3>Binary Search Tree (BST):</h3><ul><li>Left child < Node < Right child.</li><li><strong>Operations (Average):</strong> O(log n) for Search, Insert, Delete.</li><li><strong>Operations (Worst Case):</strong> O(n) - Occurs when tree becomes skewed (like a linked list, e.g., inserting sorted data).</li><li>Simple structure and operations.</li></ul><h3>Balanced BST (e.g., AVL, Red-Black Tree):</h3><ul><li>Maintain a balanced height using rotations during Insert/Delete operations.</li><li>Examples: AVL Trees (strictly balanced height), Red-Black Trees (properties guarantee balance).</li><li><strong>Operations (Guaranteed):</strong> O(log n) for Search, Insert, Delete.</li><li>More complex implementation due to rebalancing logic.</li></ul><h3>Takeaway:</h3><p>Use a standard BST when average-case O(log n) is sufficient and implementation simplicity is prioritized. Use a Balanced BST (like Red-Black Trees, commonly used in standard library maps/sets) when worst-case O(log n) performance *guarantees* are required, accepting the overhead of rebalancing operations.</p>"
    },
    {
        "title": "CS: Graph Representations",
        "content": "<h3>Concept: Storing Graph Structure</h3><p>Two primary ways to represent connections (edges) between nodes (vertices) in memory.</p><h3>Adjacency Matrix:</h3><ul><li>A V x V 2D array (or matrix), where V is the number of vertices.</li><li><code>adj[i][j] = 1</code> (or weight) if an edge exists from vertex i to vertex j, otherwise 0 (or infinity).</li><li><strong>Space:</strong> O(V^2).</li><li><strong>Check Edge (u, v):</strong> O(1) - Direct lookup.</li><li><strong>Find Neighbors of u:</strong> O(V) - Must iterate through the row for u.</li><li>Good for <strong>Dense Graphs</strong> (many edges).</li></ul><h3>Adjacency List:</h3><ul><li>An array or map where each index/key u stores a list (or set) of vertices adjacent to u.</li><li><code>adj[u] = [v1, v2, ...]</code> where (u, v1), (u, v2) are edges.</li><li><strong>Space:</strong> O(V + E), where E is the number of edges.</li><li><strong>Check Edge (u, v):</strong> O(deg(u)) in worst case for simple list, O(log V) or O(1) on average if using sorted lists or hash sets. Let's assume O(deg(u)) for a simple list.</li><li><strong>Find Neighbors of u:</strong> O(deg(u)) - Iterate through the list for u.</li><li>Good for <strong>Sparse Graphs</strong> (few edges).</li></ul><h3>Takeaway:</h3><p>Choose Adjacency Matrix for dense graphs or when frequent O(1) edge lookups are critical. Choose Adjacency List for sparse graphs or when iterating through vertex neighbors is frequent, as it's more space-efficient for sparse graphs.</p>"
    },
     {
        "title": "CS: Processes vs. Threads",
        "content": "<h3>Concept: Units of Execution</h3><p>How operating systems manage running tasks and their resources.</p><h3>Process:</h3><ul><li>An instance of a program executing (e.g., a running web browser).</li><li>Has its <strong>own dedicated memory space</strong>, file handles, stack, heap, etc. Independent resources.</li><li><strong>Heavyweight:</strong> Slower to create and context-switch between processes due to separate memory spaces.</li><li><strong>Communication:</strong> Requires Inter-Process Communication (IPC) mechanisms (pipes, sockets, shared memory) - slower but safer.</li><li>Provides isolation: If one process crashes, it doesn't directly affect others.</li></ul><h3>Thread:</h3><ul><li>A single sequence of execution *within* a process (e.g., different tabs in a browser often run in separate threads).</li><li>Shares the <strong>same memory space</strong> (heap, global variables) and resources (file handles) as other threads within the same process. Has its own stack and registers.</li><li><strong>Lightweight:</strong> Faster to create and context-switch between threads.</li><li><strong>Communication:</strong> Easy via shared memory - faster but requires careful synchronization (locks, mutexes) to avoid race conditions.</li><li>Lack of isolation: If one thread crashes, it can potentially crash the entire process.</li></ul><h3>Takeaway:</h3><p>Use Processes for strong isolation and fault tolerance (e.g., running separate applications). Use Threads for concurrency within a single application where fast communication and shared data are needed, but be mindful of synchronization challenges.</p>"
    },
    {
        "title": "CS: Binary Heaps",
        "content": "<h3>Concept: Priority Queues</h3><p>An efficient data structure to manage items with priorities, allowing fast access to the highest (or lowest) priority item.</p><h3>Binary Heap:</h3><ul><li>A <strong>complete binary tree</strong> (all levels full except possibly the last, which is filled left-to-right).</li><li>Satisfies the <strong>Heap Property</strong>:</li>    <ul><li><strong>Min-Heap:</strong> Parent node key is less than or equal to the keys of its children. The minimum element is at the root.</li><li><strong>Max-Heap:</strong> Parent node key is greater than or equal to the keys of its children. The maximum element is at the root.</li></ul><li>Often implemented using an <strong>array</strong>, leveraging the complete tree property for implicit node relationships.</li><li><strong>Operations:</strong></li>    <ul><li><strong>Insert:</strong> O(log n) - Add to end, bubble up.</li><li><strong>Extract-Min/Max:</strong> O(log n) - Remove root, replace with last element, bubble down.</li><li><strong>Peek-Min/Max:</strong> O(1) - Look at root.</li></ul></ul><h3>Takeaway:</h3><p>Binary heaps are ideal for implementing Priority Queues, offering efficient O(log n) operations for inserting and extracting the highest/lowest priority item, and O(1) access to the top element.</p>"
    },
    {
        "title": "CS: Tries (Prefix Trees)",
        "content": "<h3>Concept: Efficient String Storage & Retrieval</h3><p>A tree-like data structure specifically for storing dynamic sets or associative arrays where keys are strings.</p><h3>Trie Structure:</h3><ul><li>Nodes represent prefixes. Each node has children corresponding to the next possible characters in a key.</li><li>The root represents the empty string.</li><li>Edges are labeled with characters.</li><li>Paths from the root to certain nodes represent keys (often marked with a special 'end of word' flag).</li><li>Nodes can store associated values.</li></ul><h3>Performance:</h3><ul><li><strong>Search/Insert/Delete:</strong> O(L), where L is the length of the key. This is faster than O(L log N) for balanced BSTs or O(L) average for hash tables (due to hashing overhead, especially with long keys), and eliminates collisions.</li><li><strong>Space:</strong> Can be significant in worst case (e.g., storing all permutations), but often efficient when keys share prefixes.</li><li><strong>Prefix Search:</strong> Tries excel at finding all keys with a given prefix.</li></ul><h3>Takeaway:</h3><p>Tries are excellent for dictionary-like applications involving strings (autocomplete, spell checking, IP routing) where fast prefix-based operations and search time independent of the number of keys are crucial.</p>"
    },
    {
        "title": "CS: Skip Lists",
        "content": "<h3>Concept: Probabilistic Search Structure</h3><p>A data structure that allows O(log n) average time complexity for search, insertion, and deletion operations, built on top of sorted linked lists.</p><h3>Structure:</h3><ul><li>Consists of multiple layers of sorted linked lists.</li><li>The bottom layer is a simple sorted linked list containing all elements.</li><li>Each subsequent layer is a 'subsequence' of the layer below, containing a randomly chosen subset of nodes.</li><li>Higher layers have fewer nodes and allow for 'skipping' over large numbers of nodes in lower layers during traversal.</li></ul><h3>Operations:</h3><ul><li><strong>Search:</strong> Start at the top-left node, traverse right as long as the current node is less than the target. If the next node is too large or null, move down one layer. Repeat until the bottom layer is searched.</li><li><strong>Insertion/Deletion:</strong> Search to find the correct position in the bottom layer. Insert/delete the node. Then, probabilistically decide (e.g., coin flip) whether to 'promote' the node to the next layer, repeating for higher layers.</li></ul><h3>Takeaway:</h3><p>Skip lists offer competitive average-case performance (O(log n)) compared to balanced trees, are generally simpler to implement, and provide good concurrent access properties. Their performance guarantee is probabilistic rather than deterministic.</p>"
    },
    {
        "title": "CS: B-Trees (and B+ Trees)",
        "content": "<h3>Concept: Efficient Disk-Based Data Structures</h3><p>Trees optimized for storage systems that read and write large blocks of data (like hard drives or SSDs).</p><h3>B-Tree:</h3><ul><li>A self-balancing M-ary tree (M > 2, typically large).</li><li>Nodes can have many children (high 'fan-out').</li><li>Data records are stored in internal and leaf nodes.</li><li>Designed so that related nodes (children of a node) are likely in the same disk block, minimizing disk seeks.</li><li><strong>Search/Insert/Delete:</strong> O(log_M n). The large M means the tree is very shallow, minimizing disk block reads.</li></ul><h3>B+ Tree:</h3><ul><li>A variation commonly used in databases and file systems.</li><li><strong>All data records are stored only in the leaf nodes.</strong></li><li>Leaf nodes are linked together in a sorted list, allowing for efficient range queries.</li><li>Internal nodes only store keys (and pointers), serving as an index.</li><li>Even better for disk reads as internal nodes can pack more keys/pointers per block.</li></ul><h3>Takeaway:</h3><p>B-Trees and especially B+ Trees are fundamental for systems dealing with large datasets on disk, like databases. Their structure minimizes expensive disk I/O operations by maximizing the data/pointers stored in each block read.</p>"
    },
    {
        "title": "CS: Sorting Algorithms (QuickSort vs MergeSort)",
        "content": "<h3>Concept: Efficiently Ordering Data</h3><p>Comparing two popular O(n log n) sorting algorithms.</p><h3>QuickSort:</h3><ul><li><strong>Divide and Conquer:</strong> Picks a 'pivot', partitions the array into elements less than/greater than the pivot, recursively sorts the partitions.</li><li><strong>Average Case:</strong> O(n log n).</li><li><strong>Worst Case:</strong> O(n^2) (occurs with poor pivot choices, e.g., already sorted array).</li><li><strong>Space:</strong> O(log n) average stack space (recursive calls), O(n) worst case. Can be O(1) in-place with careful implementation.</li><li><strong>Cache Performance:</strong> Generally good due to locality of reference within partitions.</li><li>Typically faster in practice than MergeSort due to lower overhead and better cache performance.</li></ul><h3>MergeSort:</h3><ul><li><strong>Divide and Conquer:</strong> Divides array in half, recursively sorts halves, then merges sorted halves.</li><li><strong>Worst Case:</strong> O(n log n) - Performance is consistent regardless of input.</li><li><strong>Space:</strong> O(n) auxiliary space needed for merging.</li><li><strong>Stability:</strong> Is a stable sort (maintains relative order of equal elements).</li><li>Good for sorting linked lists and external sorting.</li></ul><h3>Takeaway:</h3><p>QuickSort is often preferred for arrays due to its typical speed and potential for in-place sorting, though its worst-case performance is a concern. MergeSort is chosen when stable sorting or guaranteed O(n log n) performance regardless of input is required, or for external sorting.</p>"
    },
    {
        "title": "CS: Dynamic Programming",
        "content": "<h3>Concept: Solving Complex Problems via Subproblems</h3><p>An algorithmic technique for solving optimization problems by breaking them into simpler subproblems and storing their results to avoid redundant computations.</p><h3>Core Principles:</h3><ul><li><strong>Optimal Substructure:</strong> An optimal solution to the problem contains optimal solutions to subproblems.</li><li><strong>Overlapping Subproblems:</strong> The same subproblems are encountered multiple times when solving the problem recursively.</li></ul><h3>Approaches:</h3><ul><li><strong>Memoization (Top-Down):</strong> Start from the main problem, recursively solve subproblems, store results in a cache (e.g., array, hash map) when computed for the first time, and return cached result if available. Natural recursive structure.</li><li><strong>Tabulation (Bottom-Up):</strong> Start with the smallest subproblems, compute their solutions, and store them in a table. Use table values to compute solutions for larger subproblems until the main problem's solution is reached. Typically iterative.</li></ul><h3>Takeaway:</h3><p>Dynamic Programming is applicable when a problem has optimal substructure and overlapping subproblems. It transforms exponential-time naive recursive solutions into polynomial-time solutions by systematically solving and storing results of subproblems.</p>"
    },
    {
        "title": "CS: Greedy Algorithms",
        "content": "<h3>Concept: Local Optimality</h3><p>An algorithmic paradigm that makes the locally optimal choice at each step with the hope of finding a global optimum.</p><h3>Characteristics:</h3><ul><li>Makes a choice that seems best at the moment, without regard for future consequences.</li><li>Does not reconsider previous choices.</li><li>Often simpler and faster than dynamic programming or other approaches.</li></ul><h3>Applicability:</h3><ul><li>Greedy algorithms *do not* always produce a globally optimal solution.</li><li>They work for specific problems that exhibit the 'greedy choice property' (a locally optimal choice extends to a global optimum) and optimal substructure.</li></ul><h3>Examples where Greedy Works:</h3><ul><li>Dijkstra's Shortest Path Algorithm (with non-negative weights)</li><li>Prim's and Kruskal's Minimum Spanning Tree algorithms</li><li>Huffman Coding</li><li>Activity Selection Problem</li></ul><h3>Examples where Greedy Fails:</h3><ul><li>Coin Change Problem (depending on coin denominations) - DP is needed for optimal solution.</li><li>Travelling Salesperson Problem</li></ul><h3>Takeaway:</h3><p>Greedy algorithms are simple and fast but are only suitable for problems where the greedy choice property holds. Verifying this property is crucial before applying a greedy approach.</p>"
    },
    {
        "title": "CS: Backtracking",
        "content": "<h3>Concept: Exploring Possibilities Systematically</h3><p>An algorithmic technique for finding solutions to computational problems, particularly constraint satisfaction problems, by trying to build a solution incrementally and 'backtracking' (undoing the last step) when a partial solution cannot be extended to a complete solution.</p><h3>Process:</h3><ul><li>Build a solution step by step (e.g., assigning a value to a variable, placing an item).</li><li>At each step, check if the current partial solution violates any constraints.</li><li>If a constraint is violated, abandon this path and backtrack to the previous step to try a different choice.</li><li>If a partial solution is valid, recursively proceed to the next step.</li><li>If a complete, valid solution is found, record it (or stop).</li></ul><h3>Analogy:</h3><p>Exploring a maze. You follow a path until you hit a dead end, then go back to the last junction and try another path.</p><h3>Use Cases:</h3><ul><li>Solving puzzles (Sudoku, N-Queens)</li><li>Combinatorial problems (finding subsets, permutations, combinations)</li><li>Constraint satisfaction problems</li><li>Parsing</li></ul><h3>Takeaway:</h3><p>Backtracking is a form of depth-first search on the state-space tree of the problem. It is effective for searching for solutions in a large solution space by pruning branches that cannot lead to a valid answer, though it can still be exponential in the worst case.</p>"
    },
    {
        "title": "CS: BFS vs. DFS (Graph Traversal)",
        "content": "<h3>Concept: Exploring Graphs/Trees</h3><p>Two fundamental algorithms for visiting all nodes in a graph or tree.</p><h3>Breadth-First Search (BFS):</h3><ul><li>Explores level by level. Visits all neighbors of a node before moving to the next level.</li><li>Uses a <strong>Queue</strong> to manage nodes to visit.</li><li>Guaranteed to find the <strong>shortest path in terms of number of edges</strong> in an unweighted graph.</li><li>Can require more memory than DFS (O(V) in worst case for queue).</li><li>Typically implemented iteratively.</li></ul><h3>Depth-First Search (DFS):</h3><ul><li>Explores as deeply as possible along each branch before backtracking.</li><li>Uses a <strong>Stack</strong> (explicit or via recursion call stack) to manage nodes to visit.</li><li>Useful for detecting cycles, topological sorting, and exploring connected components.</li><li>Memory usage is O(height of tree/graph) in recursion stack, O(V+E) for explicit stack + visited set.</li><li>Can be implemented recursively or iteratively.</li></ul><h3>Takeaway:</h3><p>Use BFS for finding shortest paths (edge count) or when you need to explore nodes level by level. Use DFS for exploring the full depth of a branch, detecting cycles, or when memory is constrained (in deep graphs/trees).</p>"
    },
    {
        "title": "CS: Dijkstra vs. Bellman-Ford (Shortest Path)",
        "content": "<h3>Concept: Finding Minimum Cost Paths</h3><p>Two classic algorithms for finding the shortest paths from a single source vertex to all other vertices in a graph.</p><h3>Dijkstra's Algorithm:</h3><ul><li>Finds shortest paths in graphs with <strong>non-negative edge weights</strong>.</li><li>A greedy algorithm. Maintains a set of visited nodes and a priority queue of unvisited nodes, always selecting the unvisited node with the current smallest known distance.</li><li><strong>Time Complexity:</strong> O(E log V) with a binary heap, or O(E + V log V) with a Fibonacci heap.</li><li>Does <strong>not</strong> work correctly with negative edge weights.</li></ul><h3>Bellman-Ford Algorithm:</h3><ul><li>Finds shortest paths in graphs that may contain <strong>negative edge weights</strong>.</li><li>Relaxes all edges V-1 times. If relaxing edges the V-th time still reduces a path length, a <strong>negative cycle</strong> exists (and shortest paths are undefined).</li><li><strong>Time Complexity:</strong> O(V * E). Slower than Dijkstra for graphs with non-negative weights.</li><li>Can detect negative cycles reachable from the source.</li></ul><h3>Takeaway:</h3><p>Use Dijkstra's Algorithm for faster shortest path calculations when all edge weights are non-negative. Use Bellman-Ford when the graph may contain negative edge weights, accepting the higher time complexity, and also to detect negative cycles.</p>"
    },
     {
        "title": "CS: Prim vs. Kruskal (MST)",
        "content": "<h3>Concept: Finding Minimum Spanning Trees</h3><p>Two greedy algorithms to find a Minimum Spanning Tree (MST) in a connected, undirected graph with weighted edges.</p><h3>Minimum Spanning Tree (MST):</h3><p>A subset of the edges that connects all the vertices together, without any cycles, and with the minimum possible total edge weight.</p><h3>Prim's Algorithm:</h3><ul><li>Starts with an arbitrary vertex and grows the MST by adding the minimum-weight edge connecting a vertex in the current MST to a vertex outside the MST.</li><li>Uses a <strong>priority queue</strong> to efficiently find the next minimum-weight edge.</li><li>Often implemented on an adjacency list representation.</li><li><strong>Time Complexity:</strong> O(E log V) or O(E + V log V) with a Fibonacci heap.</li></ul><h3>Kruskal's Algorithm:</h3><ul><li>Starts with an empty MST and adds edges in increasing order of weight, skipping edges that would form a cycle with already added edges.</li><li>Uses a <strong>Disjoint Set Union (DSU)</strong> data structure to efficiently check if adding an edge creates a cycle and to merge connected components.</li><li>Often implemented by sorting edges first.</li><li><strong>Time Complexity:</strong> O(E log E) or O(E log V) (since E is at most V^2, log E is O(log V^2) = O(log V), or more precisely, sorting dominates at O(E log E)).</li></ul><h3>Takeaway:</h3><p>Both Prim's and Kruskal's find an MST. Prim's expands from a vertex, good for dense graphs or when a starting vertex is specified. Kruskal's adds edges globally based on weight, often simpler to understand and implement, and efficient for sparse graphs.</p>"
    },
    {
        "title": "CS: Recursion vs. Iteration",
        "content": "<h3>Concept: Repeated Execution</h3><p>Two fundamental programming approaches for performing repetitive tasks.</p><h3>Recursion:</h3><ul><li>A function calls itself, breaking a problem down into smaller, self-similar subproblems.</li><li>Requires a <strong>base case</strong> to stop the recursion.</li><li>Uses the <strong>call stack</strong> to manage state (parameters, local variables) for each recursive call.</li><li>Can be elegant and mirrors mathematical definitions for problems with recursive structure (e.g., factorial, tree traversals).</li><li>Risk of <strong>Stack Overflow</strong> for deep recursion if not optimized (Tail Call Optimization).</li><li>Overhead due to function calls and stack management.</li></ul><h3>Iteration:</h3><ul><li>Uses loops (<code>for</code>, <code>while</code>) to repeat a block of code.</li><li>State is managed explicitly using loop variables and data structures.</li><li>Generally uses less memory than recursion (no function call stack per iteration).</li><li>Often more intuitive for simple repetitions.</li><li>Can be less readable for problems with deeply nested or complex recursive structure.</li></ul><h3>Takeaway:</h3><p>Choose Recursion for problems with an inherent recursive structure (trees, graphs, divide-and-conquer) where it leads to cleaner code, but be mindful of stack limits. Choose Iteration for simple repetitions or when performance/memory usage is critical and the problem can be naturally expressed with loops.</p>"
    },
     {
        "title": "CS: Virtual Memory",
        "content": "<h3>Concept: Illusion of Infinite Memory</h3><p>An OS technique that provides processes with the illusion of having a large, contiguous block of memory, even if physical RAM is fragmented or limited, by using disk space as an extension of RAM.</p><h3>How it Works:</h3><ul><li>Divides a process's logical address space and physical memory into fixed-size chunks (<strong>pages</strong> for logical, <strong>frames</strong> for physical).</li><li>Maintains a <strong>Page Table</strong> for each process, mapping logical page numbers to physical frame numbers.</li><li>Allows parts of a process to reside in RAM while others are temporarily stored on disk (<strong>swapping</strong>).</li><li>When a process accesses a page not in RAM, a <strong>Page Fault</strong> occurs. The OS loads the required page from disk into a free frame in RAM (possibly evicting another page).</li></ul><h3>Benefits:</h3><ul><li>Allows processes larger than physical memory to run.</li><li>Enables memory protection between processes (each has its own isolated address space).</li><li>Simplifies memory management for programmers.</li><li>Facilitates efficient memory sharing between processes.</li></ul><h3>Takeaway:</h3><p>Virtual Memory is a cornerstone of modern OS, providing memory isolation, allowing multitasking with limited RAM, and enabling programs larger than physical memory. It relies heavily on hardware support (MMU) and comes with potential performance costs (thrashing) if excessive swapping occurs.</p>"
    },
    {
        "title": "CS: Concurrency Issues (Deadlock vs. Race Condition)",
        "content": "<h3>Concept: Problems in Parallel Execution</h3><p>Common pitfalls when multiple processes or threads access shared resources concurrently.</p><h3>Race Condition:</h3><ul><li>Occurs when the output of a concurrent program depends on the unpredictable order in which operations by multiple processes/threads are interleaved.</li><li>Results from multiple entities accessing and modifying shared data without proper synchronization.</li><li>The 'race' is between processes/threads to access/modify the shared resource first.</li><li>Can lead to corrupted data or incorrect program behavior.</li><li><strong>Mitigation:</strong> Use synchronization primitives like mutexes, semaphores, locks, or atomic operations to ensure controlled access to shared resources.</li></ul><h3>Deadlock:</h3><ul><li>A state where two or more processes are blocked indefinitely, waiting for each other to release resources that they hold.</li><li>Requires four necessary conditions (Coffman Conditions) to occur simultaneously: <strong>Mutual Exclusion, Hold and Wait, No Preemption, Circular Wait</strong>.</li><li>Processes cannot proceed because each is waiting for a resource held by another process in the cycle.</li><li><strong>Mitigation:</strong> Prevention (negate one of the four conditions), Avoidance (e.g., Banker's algorithm), Detection (find cycles in resource allocation graph) & Recovery (preempt resources, terminate processes).</li></ul><h3>Takeaway:</h3><p>Race conditions are about timing-dependent data corruption due to unsynchronized access to shared data. Deadlocks are about processes being stuck indefinitely, waiting for resources held by others. Both require careful design and synchronization in concurrent systems.</p>"
    },
    {
        "title": "CS: ACID Properties (Databases)",
        "content": "<h3>Concept: Transaction Reliability</h3><p>A set of properties guaranteeing that database transactions are processed reliably.</p><h3>Transaction:</h3><p>A single unit of work that must be completed entirely or not at all.</p><h3>ACID Properties:</h3><ul><li><strong>Atomicity:</strong> A transaction is treated as a single, indivisible unit. Either all operations within it are completed successfully, or none of them are (it's rolled back). Prevents partial updates.</li><li><strong>Consistency:</strong> A transaction brings the database from one valid state to another valid state. It must not violate any database invariants (rules, constraints).</li><li><strong>Isolation:</strong> Concurrent transactions do not interfere with each other. Each transaction appears to run as if it is the only transaction operating on the database, even if others are executing concurrently. Achieved via locking or multiversion concurrency control.</li><li><strong>Durability:</strong> Once a transaction is committed, its changes are permanent and survive system failures (power loss, crashes). Typically achieved by writing changes to persistent storage (disk, transaction logs).</li></ul><h3>Takeaway:</h3><p>Adhering to ACID properties is fundamental for building reliable and robust database systems, ensuring data integrity and consistency even in the presence of concurrent access and failures. They provide a strong guarantee about the outcome of transactions.</p>"
    },
    {
        "title": "CS: Database Indexing",
        "content": "<h3>Concept: Speeding up Data Retrieval</h3><p>A data structure technique used to quickly locate and access data within a database table without having to scan the entire table.</p><h3>How it Works:</h3><ul><li>An index is typically a separate data structure (most commonly a <strong>B+ Tree</strong>) that stores a small subset of the data from a table column(s) along with pointers to the full data rows.</li><li>When you query the database based on an indexed column, the database system first searches the index (which is fast due to its structure and smaller size) to find the location of the relevant rows, then retrieves the full rows directly.</li></ul><h3>Benefits:</h3><ul><li>Significantly speeds up read operations (<code>SELECT</code>) for queries using indexed columns in <code>WHERE</code> clauses, <code>JOIN</code> conditions, or <code>ORDER BY</code> clauses.</li></ul><h3>Drawbacks:</h3><ul><li>Requires extra storage space for the index itself.</li><li>Slows down write operations (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) because the index must also be updated whenever the underlying data changes.</li><li>Indexes need to be chosen carefully based on query patterns; too many indexes can hurt write performance without providing commensurate read benefits.</li></ul><h3>Takeaway:</h3><p>Indexing is a crucial database optimization technique for improving read performance, especially on large tables. It involves a trade-off between read speed and write speed/storage space and should be applied judiciously to frequently queried columns.</p>"
    },
    {
        "title": "CS: TCP vs. UDP",
        "content": "<h3>Concept: Transport Layer Protocols</h3><p>Two core protocols in the internet protocol suite (part of the Transport Layer) for sending data between applications.</p><h3>Transmission Control Protocol (TCP):</h3><ul><li><strong>Connection-Oriented:</strong> Establishes a connection (handshake) before sending data.</li><li><strong>Reliable:</strong> Guarantees delivery of data, retransmits lost packets, orders packets correctly. Uses acknowledgments and sequence numbers.</li><li><strong>Flow Control:</strong> Prevents sender from overwhelming receiver.</li><li><strong>Congestion Control:</strong> Manages network traffic to avoid collapse.</li><li><strong>Header Overhead:</strong> Larger header due to more features.</li><li><strong>Use Cases:</strong> Web browsing (HTTP/S), Email (SMTP), File Transfer (FTP) - where reliability is critical.</li></ul><h3>User Datagram Protocol (UDP):</h3><ul><li><strong>Connectionless:</strong> Sends data packets (datagrams) without establishing a connection. 'Fire and forget'.</li><li><strong>Unreliable:</strong> Does not guarantee delivery, order, or duplicate prevention.</li><li><strong>No Flow/Congestion Control:</strong> Sends data as fast as the application provides it.</li><li><strong>Header Overhead:</strong> Minimal header.</li><li><strong>Speed:</strong> Faster due to less overhead and no waiting for acknowledgments.</li><li><strong>Use Cases:</strong> Streaming (audio/video), Online Gaming, DNS, VoIP - where speed and low latency are prioritized over guaranteed delivery of every single packet.</li></ul><h3>Takeaway:</h3><p>Choose TCP when reliable, ordered delivery is paramount, accepting the overhead. Choose UDP when speed and low latency are more important than guaranteed delivery, and the application can handle potential packet loss or reordering.</p>"
    },
    {
        "title": "CS: Compilation vs. Interpretation",
        "content": "<h3>Concept: Executing Code</h3><p>Two primary approaches for translating and running source code.</p><h3>Compilation:</h3><ul><li>Source code is translated into machine code (or intermediate bytecode) <strong>once</strong> by a compiler before execution.</li><li>The output is an executable file that can be run directly by the operating system/hardware.</li><li><strong>Process:</strong> Source Code -> Compiler -> Object Code -> Linker -> Executable.</li><li><strong>Execution Speed:</strong> Generally faster because the translation is done upfront.</li><li><strong>Development Cycle:</strong> Slower edit-compile-run cycle.</li><li><strong>Error Detection:</strong> Errors (like syntax errors) are typically caught during compilation.</li><li><strong>Examples:</strong> C, C++, Java (compiled to bytecode), Go.</li></ul><h3>Interpretation:</h3><ul><li>Source code is translated and executed <strong>line by line</strong> or statement by statement by an interpreter at runtime.</li><li>No separate executable file is produced.</li><li><strong>Process:</strong> Source Code -> Interpreter -> Execution.</li><li><strong>Execution Speed:</strong> Generally slower due to the runtime translation overhead.</li><li><strong>Development Cycle:</strong> Faster edit-run cycle.</li><li><strong>Error Detection:</strong> Errors may only be found when the corresponding line of code is executed.</li><li><strong>Examples:</strong> Python, JavaScript, Ruby, PHP.</li></ul><h3>Hybrid Approaches (JIT):</h3><p>Some languages/runtimes use Just-In-Time (JIT) compilation, compiling code during execution for performance benefits (e.g., Java JVM, V8 for JavaScript).</p><h3>Takeaway:</h3><p>Compilation favors execution speed and early error detection, suitable for performance-critical applications. Interpretation favors faster development cycles and flexibility, suitable for scripting and rapid prototyping. Hybrid approaches aim to combine the benefits.</p>"
    },
    {
        "title": "CS: Garbage Collection",
        "content": "<h3>Concept: Automatic Memory Management</h3><p>A form of automatic memory management that attempts to reclaim memory occupied by objects that are no longer reachable or used by the program.</p><h3>Manual Memory Management:</h3><ul><li>Programmer is responsible for allocating and deallocating memory (e.g., <code>malloc</code>/<code>free</code> in C, <code>new</code>/<code>delete</code> in C++).</li><li>Risk of <strong>memory leaks</strong> (unused memory not deallocated) and <strong>dangling pointers</strong> (pointers to deallocated memory).</li></ul><h3>Garbage Collection:</h3><ul><li>The runtime environment (JVM, .NET CLR, Python interpreter, etc.) tracks memory allocation and usage.</li><li>Periodically identifies 'garbage' (memory objects no longer referenced by the running program).</li><li>Reclaims this memory, making it available for future allocations.</li></ul><h3>Common Algorithms:</h3><ul><li><strong>Reference Counting:</strong> Keep a count of references to each object. When count reaches zero, object is garbage. Simple, but cannot handle reference cycles.</li><li><strong>Mark-and-Sweep:</strong> Two phases: Mark reachable objects starting from root references, then Sweep (deallocate) all unmarked objects. Handles cycles.</li><li><strong>Copying Collectors:</strong> Divide memory into sections, copy live objects from one section to another during collection. Naturally defragments memory.</li><li><strong>Generational GC:</strong> Based on the observation that most objects die young. Divides heap into 'generations' and collects younger generations more frequently.</li></ul><h3>Takeaway:</h3><p>Garbage Collection simplifies programming by automating memory deallocation, reducing memory errors like leaks and dangling pointers. However, it introduces runtime overhead and potential pauses ('stop-the-world' events) during collection cycles, which can be optimized by different algorithms and tuning.</p>"
    },
    {
        "title": "CS: Big O Notation (Advanced Considerations)",
        "content": "<h3>Concept: Analyzing Algorithm Efficiency (Beyond Basics)</h3><p>Quantifying how the running time or space requirements of an algorithm grow as the input size increases.</p><h3>Basics Refresher:</h3><ul><li>O(1): Constant time (e.g., array access)</li><li>O(log n): Logarithmic time (e.g., binary search)</li><li>O(n): Linear time (e.g., traversing a list)</li><li>O(n log n): Log-linear time (e.g., efficient sorts like MergeSort, QuickSort average)</li><li>O(n^2): Quadratic time (e.g., nested loops, bubble sort)</li><li>O(2^n): Exponential time (e.g., brute-force solutions to some problems)</li></ul><h3>Beyond Asymptotic Growth:</h3><ul><li><strong>Constants Matter (Sometimes):</strong> O(n) is asymptotically better than O(n^2), but for small 'n', an algorithm with a large constant factor might be slower than one with a smaller factor.</li><li><strong>Average vs. Worst Case:</strong> Big O typically refers to the worst case, but average case (e.g., QuickSort O(n log n) average vs O(n^2) worst) is also important.</li><li><strong>Space Complexity:</strong> Big O applies to memory usage too. O(1) space is ideal.</li><li><strong>Amortized Analysis:</strong> Average performance of an operation over a sequence of operations, e.g., O(1) amortized time for dynamic array append despite occasional O(n) resizes.</li><li><strong>Little O (o):</strong> Strict inequality. f(n) is o(g(n)) if lim(f(n)/g(n)) as n->inf = 0. Means f grows *strictly slower* than g.</li><li><strong>Theta (Θ):</strong> Tight bound. f(n) is Θ(g(n)) if it's both O(g(n)) and Ω(g(n)). Means f grows at the *same rate* as g.</li><li><strong>Omega (Ω):</strong> Lower bound. f(n) is Ω(g(n)) if f grows *at least as fast* as g.</li></ul><h3>Takeaway:</h3><p>Big O (O) provides an upper bound on growth. For more precise analysis, consider Ω (lower bound) and Θ (tight bound). Understand average vs. worst case, and remember that constants and specific input distributions can be relevant in practice, even if they don't change the Big O class.</p>"
    },
    {
        "title": "CS: P vs. NP",
        "content": "<h3>Concept: Problem Complexity Classes</h3><p>A fundamental open question in theoretical computer science concerning the relationship between two classes of computational problems.</p><h3>Complexity Class P:</h3><ul><li>Contains decision problems (problems with a 'yes' or 'no' answer).</li><li>Problems in P are those that can be solved by a <strong>deterministic Turing machine in polynomial time</strong> (i.e., O(n^k) for some constant k, where n is the input size).</li><li>These problems are considered 'tractable' or 'easy' to solve algorithmically.</li><li>Examples: Sorting, searching, finding shortest path in unweighted graph.</li></ul><h3>Complexity Class NP:</h3><ul><li>Contains decision problems.</li><li>Problems in NP are those for which a potential solution can be <strong>verified in polynomial time</strong> by a deterministic Turing machine. The 'N' stands for 'Non-deterministic' (referring to a non-deterministic Turing machine which can solve them in polynomial time).</li><li>Finding a solution might be hard, but checking if a proposed solution is correct is easy.</li><li>Examples: Satisfiability (SAT), Traveling Salesperson Problem (decision version), Subset Sum.</li></ul><h3>The P vs. NP Question:</h3><p>Is P = NP? Does every problem whose solution can be *verified* quickly also have a solution that can be *found* quickly? Most computer scientists believe P ≠ NP.</p><h3>NP-Complete:</h3><p>A subset of NP problems that are the 'hardest' in NP. Any problem in NP can be reduced to an NP-Complete problem in polynomial time. If an NP-Complete problem could be solved in polynomial time, then P would equal NP.</p><h3>Takeaway:</h3><p>P problems are those solvable in polynomial time. NP problems are those whose solutions are verifiable in polynomial time. The P=NP question asks if finding is as easy as verifying. NP-Complete problems are central to this debate, being the hardest problems in NP. Proving P=NP or P≠NP would be a monumental breakthrough.</p>"
    },
    {
        "title": "CS: File System Basics",
        "content": "<h3>Concept: Organizing Data on Storage</h3><p>The method and data structures that an operating system uses to control how data is stored and retrieved on a storage device (like a hard drive, SSD, USB drive).</p><h3>Key Functions:</h3><ul><li><strong>Organizing Data:</strong> Provides a hierarchical structure (directories/folders) to group files.</li><li><strong>Managing Space:</strong> Tracks used and free space on the device. Allocates space for files.</li><li><strong>Providing Metadata:</strong> Stores information about files (name, size, creation/modification date, permissions, owner).</li><li><strong>Ensuring Data Integrity:</strong> Protecting data from corruption (though reliability varies by filesystem).</li><li><strong>Access Control:</strong> Implementing permissions to restrict who can read/write/execute files.</li></ul><h3>Core Components (Conceptual):</h3><ul><li><strong>Boot Block:</strong> Contains bootstrapping code to load the OS.</li><li><strong>Superblock:</strong> Contains metadata about the entire file system (size, block size, free block list, etc.).</li><li><strong>Inode/File Entry Structure:</strong> Stores metadata for each file/directory, including pointers to its data blocks. (Often `inode` in Unix-like, `file entry` in others).</li><li><strong>Data Blocks:</strong> The actual content of files is stored in fixed-size blocks on the device.</li><li><strong>Directory Structure:</strong> Stores names and pointers (like inode numbers) to the files and subdirectories within it.</li></ul><h3>Takeaway:</h3><p>File systems are essential for persistent data storage, providing structure, management, and access control on storage devices. Different file systems (e.g., ext4, NTFS, APFS) implement these concepts with varying structures, performance characteristics, and feature sets.</p>"
    },
    {
        "title": "CS: Object-Oriented vs. Functional Programming",
        "content": "<h3>Concept: Programming Paradigms</h3><p>Different styles or ways of thinking about structuring programs.</p><h3>Object-Oriented Programming (OOP):</h3><ul><li><strong>Focus:</strong> Data and the methods that operate on that data are bundled together into <strong>objects</strong>.</li><li><strong>Core Concepts:</strong> Encapsulation (bundling data/methods), Inheritance (creating new classes from existing ones), Polymorphism (objects of different classes responding to the same method call).</li><li><strong>State:</strong> Objects often have internal state that can change.</li><li><strong>Approach:</strong> Model the world as interacting objects. Imperative style is common (changing state).</li><li><strong>Examples:</strong> Java, C++, Python, C# (support OOP).</li></ul><h3>Functional Programming (FP):</h3><ul><li><strong>Focus:</strong> Computation is treated as the evaluation of <strong>mathematical functions</strong>.</li><li><strong>Core Concepts:</strong> First-class functions (functions as data), Immutability (data is generally not changed after creation), Pure functions (always return same output for same input, no side effects), Higher-order functions (functions that take/return other functions).</li><li><strong>State:</strong> Minimize mutable state; prefer creating new data from old.</li><li><strong>Approach:</strong> Define what needs to be computed using expressions and function composition. Declarative style is common.</li><li><strong>Examples:</strong> Haskell (pure FP), Lisp, Clojure, Scala, F#, JavaScript (supports FP).</li></ul><h3>Takeaway:</h3><p>OOP organizes code around data and objects with mutable state, emphasizing encapsulation and inheritance. FP organizes code around functions and immutable data, emphasizing pure functions and composition. Many modern languages support elements of both paradigms, allowing developers to choose the best approach for different parts of a system.</p>"
    }
]
