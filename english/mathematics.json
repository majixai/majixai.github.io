[
    {
        "title": "Math: Linear Independence (Linear Algebra)",
        "content": "<h3>Concept: Non-Redundancy in Vectors</h3><p>A set of vectors <code>{v1, v2, ..., vk}</code> is <strong>linearly independent</strong> if the only way to form a linear combination of these vectors that results in the zero vector is when all the scalar coefficients are zero. It signifies that no vector in the set can be expressed as a combination of the others.</p><h3>Defining Equation & Trivial Solution:</h3><p>Consider the linear combination:<br><code>c1*v1 + c2*v2 + ... + ck*vk = 0</code> (where <code>0</code> is the zero vector)</p><p>This set of vectors is linearly independent <em>if and only if</em> the <em>only possible values</em> for the scalar coefficients (<code>c1, c2, ..., ck</code>) that satisfy this equation are:<br><code>c1 = 0, c2 = 0, ..., ck = 0</code></p><p>This unique solution where all coefficients are zero is called the <strong>trivial solution</strong>.</p><h3>Intuition & Geometric Interpretation:</h3><p>Think of the vectors as directions. If a set of vectors is linearly independent, adding any multiple of one vector will move you in a direction that *cannot* be achieved by only combining multiples of the other vectors. Each vector contributes uniquely to the set's span.</p><ul><li>In 2D: Two vectors are linearly independent if they are not parallel (one is not a scalar multiple of the other). They 'point' in different directions, allowing you to reach any point in the 2D plane (if they are non-zero).</li><li>In 3D: Three vectors are linearly independent if they don't all lie in the same plane passing through the origin.</li></ul><p>Each linearly independent vector adds a new dimension to the space they span.</p><h3>Contrast (Linear Dependence):</h3><p>If there exists *any* solution to the equation <code>c1*v1 + ... + ck*vk = 0</code> where <em>at least one</em> coefficient <code>ci</code> is <em>not</em> zero (a <strong>non-trivial solution</strong>), the set is <strong>linearly dependent</strong>.</p><p>If a set is linearly dependent, it means at least one vector <code>vi</code> in the set *can* be written as a linear combination of the *other* vectors. For example, if <code>c1*v1 + c2*v2 = 0</code> with <code>c1 ≠ 0</code>, then <code>v1 = (-c2/c1)*v2</code>, meaning <code>v1</code> is a scalar multiple of <code>v2</code> (they are collinear). This indicates redundancy in the set.</p><h3>Checking for Linear Independence:</h3><p>To check if a set of vectors is linearly independent, you can form a matrix where the columns (or rows) are the vectors. Then, solve the system of linear equations represented by <code>Ac = 0</code> (where A is the matrix and c is the vector of coefficients).</p><ul><li>The set is linearly independent if and only if the matrix A has <strong>full column rank</strong> (or its determinant is non-zero if it's a square matrix), which implies the system <code>Ac = 0</code> has only the trivial solution.</li><li>Alternatively, use Gaussian elimination to row reduce the matrix. The columns corresponding to pivot positions are linearly independent. If the number of pivot columns equals the number of vectors, they are linearly independent.</li></ul><h3>Takeaway:</h3><p>Linear independence is a core concept in Linear Algebra. It is fundamental for understanding the <strong>span</strong> of a set of vectors and for defining a <strong>basis</strong> – a minimal set of vectors that can generate an entire vector space. Linearly independent vectors are essential building blocks that are not redundant.</p>"
    },
    {
        "title": "Math: Eigenvalues & Eigenvectors",
        "content": "<h3>Concept: Stretching/Shrinking Directions</h3><p>For a square matrix A, an <strong>eigenvector</strong> (<code>v</code>, non-zero) is a vector that, when multiplied by A, only changes in magnitude (stretched or shrunk), not direction. The factor by which it's scaled is the corresponding <strong>eigenvalue</strong> (<code>λ</code>).</p><h3>Defining Equation:</h3><p><code>A * v = λ * v</code><br>where:<br><code>A</code> is the square matrix<br><code>v</code> is the eigenvector (<code>v ≠ 0</code>)<br><code>λ</code> is the eigenvalue (a scalar)</p><h3>Finding Eigenvalues:</h3><p>Rearrange the equation: <code>A * v - λ * v = 0</code> => <code>(A - λI) * v = 0</code>, where <code>I</code> is the identity matrix.</p><p>For this system to have a non-trivial solution (<code>v ≠ 0</code>), the matrix <code>(A - λI)</code> must be singular, meaning its determinant is zero:<br><code>det(A - λI) = 0</code></p><p>This equation is called the <strong>characteristic equation</strong>. Solving it for <code>λ</code> gives the eigenvalues.</p><h3>Finding Eigenvectors:</h3><p>Once an eigenvalue <code>λ</code> is found, substitute it back into <code>(A - λI) * v = 0</code> and solve the system for <code>v</code>. The set of solutions for <code>v</code> (excluding the zero vector) forms the <strong>eigenspace</strong> for that eigenvalue. Any non-zero vector in the eigenspace is a valid eigenvector for <code>λ</code>.</p><h3>Geometric Intuition:</h3><p>Eigenvectors represent the inherent directions within a linear transformation (represented by the matrix A) that are invariant under the transformation, only being scaled by the eigenvalue. Think of them as the 'axes' of the transformation.</p><h3>Takeaway:</h3><p>Eigenvalues and eigenvectors reveal fundamental properties of linear transformations, describing directions that remain unchanged (scaled) and the scaling factor. They are crucial for diagonalization, solving systems of ODEs, principal component analysis (PCA), and quantum mechanics.</p>"
    },
     {
        "title": "Math: Singular Value Decomposition (SVD)",
        "content": "<h3>Concept: Generalizing Eigen decomposition</h3><p>SVD is a powerful matrix factorization technique that decomposes <em>any</em> matrix (rectangular or square) into three other matrices. It's a generalization of diagonalization/eigen decomposition applicable even to non-square matrices.</p><h3>The Decomposition:</h3><p>Any <code>m x n</code> matrix <code>A</code> can be decomposed as:<br><code>A = U * Σ * V^T</code></p><p>where:</p><ul><li><code>U</code> is an <code>m x m</code> orthogonal matrix whose columns are the left singular vectors.</li><li><code>Σ</code> (Sigma) is an <code>m x n</code> diagonal matrix containing the <strong>singular values</strong> on its main diagonal. The singular values are the square roots of the non-zero eigenvalues of both <code>A^T * A</code> and <code>A * A^T</code>, sorted in descending order.</li><li><code>V^T</code> is the transpose of an <code>n x n</code> orthogonal matrix <code>V</code>, whose columns are the right singular vectors.</li></ul><h3>Geometric Interpretation:</h3><p>SVD states that any linear transformation (matrix A) can be broken down into three geometric operations:</p><ol><li>A rotation or reflection (<code>V^T</code>).</li><li>A scaling along coordinate axes by factors given by the singular values (<code>Σ</code>).</li><li>Another rotation or reflection (<code>U</code>).</li></ol><h3>Key Properties & Uses:</h3><ul><li>Applicable to any matrix, unlike eigen decomposition (which requires square matrices).</li><li>The number of non-zero singular values equals the <strong>rank</strong> of the matrix A.</li><li>Provides insight into the matrix's structure and properties.</li><li>Crucial for dimensionality reduction (like PCA), matrix approximation, noise reduction, solving linear least squares, recommender systems, and image compression.</li></ul><h3>Takeaway:</h3><p>SVD is a fundamental tool in linear algebra and data science for decomposing any matrix into rotations and scalings. It reveals the rank and structure of a matrix and is widely used for data analysis, dimension reduction, and solving linear systems.</p>"
    },
    {
        "title": "Math: Kernel (Null Space) & Image (Column Space)",
        "content": "<h3>Concept: Input & Output Spaces of a Linear Transformation</h3><p>For a linear transformation <code>T: V -> W</code> (or its matrix representation <code>A</code>), the kernel and image describe specific subspaces related to the transformation's effect on vectors.</p><h3>Kernel (Null Space), N(A):</h3><ul><li>The set of all vectors <code>v</code> in the domain (V) that are mapped to the <strong>zero vector</strong> in the codomain (W).<br><code>N(A) = { v ∈ V | A * v = 0 }</code></li><li>It's a subspace of the domain V.</li><li>Finding the kernel involves solving the homogeneous system <code>A * v = 0</code> (finding the null space of the matrix A).</li><li>The <strong>nullity</strong> is the dimension of the kernel.</li></ul><h3>Image (Column Space), Im(A) or C(A):</h3><ul><li>The set of all possible output vectors in the codomain (W) that can be reached by applying the transformation A to *some* vector in the domain V.<br><code>Im(A) = { A * v | v ∈ V }</code></li><li>It's the subspace spanned by the <strong>column vectors</strong> of the matrix A.</li><li>It's a subspace of the codomain W.</li><li>Finding a basis for the image involves identifying the linearly independent columns of A (e.g., columns corresponding to pivot positions after row reduction).</li><li>The <strong>rank</strong> is the dimension of the image.</li></ul><h3>Takeaway:</h3><p>The kernel tells you which input vectors get 'crushed' to zero by the transformation (information lost). The image tells you the set of all possible output vectors the transformation can produce (the reach of the transformation). These subspaces are fundamental to understanding linear transformations and are linked by the Rank-Nullity Theorem.</p>"
    },
     {
        "title": "Math: The Rank-Nullity Theorem",
        "content": "<h3>Concept: Relating Kernel and Image Dimensions</h3><p>A fundamental theorem in linear algebra that connects the dimensions of the kernel (null space) and the image (column space) of a linear transformation or matrix.</p><h3>The Theorem Statement:</h3><p>For a linear transformation <code>T: V -> W</code>, where V is a finite-dimensional vector space,<br><code>dim(Kernel(T)) + dim(Image(T)) = dim(V)</code></p><p>If A is the matrix representation of T (an <code>m x n</code> matrix, where <code>n = dim(V)</code>), this translates to:<br><code>Nullity(A) + Rank(A) = n</code> (the number of columns in A, which is the dimension of the domain)</p><h3>Intuition:</h3><p>Think of the transformation A mapping vectors from an n-dimensional space (the domain) to an m-dimensional space (the codomain). Some information about the input vectors is 'lost' if they are mapped to zero (the kernel). The information that is 'preserved' dictates the dimension of the output space (the image).</p><p>The theorem states that the dimensions of the 'lost information space' (Kernel) and the 'preserved information space' (Image) must add up to the total dimension of the initial space (Domain).</p><h3>Example:</h3><p>If a 3x5 matrix A has a 2-dimensional kernel (nullity = 2), it means 2 dimensions of the input space (R^5) are mapped to zero. The Rank-Nullity Theorem tells us the dimension of the image (column space) must be 5 - 2 = 3. The rank is 3.</p><h3>Takeaway:</h3><p>The Rank-Nullity Theorem provides a powerful relationship between the size of the input space, the 'information loss' (nullity), and the 'information preserved' or 'reach' (rank) of a linear transformation. It's a key tool for analyzing the structure and properties of matrices and systems of linear equations.</p>"
    },
    {
        "title": "Math: Change of Basis",
        "content": "<h3>Concept: Representing Vectors in Different Coordinate Systems</h3><p>Vectors and linear transformations are often represented relative to a chosen <strong>basis</strong> (a set of linearly independent vectors spanning the space). A 'change of basis' involves converting the coordinates of a vector or the matrix of a transformation from one basis to another.</p><h3>Basis Vector and Coordinates:</h3><p>If <code>B = {b1, b2, ..., bn}</code> is a basis for a vector space V, any vector <code>v</code> in V can be written uniquely as a linear combination of the basis vectors:<br><code>v = c1*b1 + c2*b2 + ... + cn*bn</code></p><p>The scalars <code>c1, c2, ..., cn</code> are the coordinates of <code>v</code> with respect to the basis B, denoted as <code>[v]_B</code>.</p><h3>Change of Basis Matrix:</h3><p>Suppose we have two bases for V: <code>B = {b1, ..., bn}</code> and <code>C = {c1, ..., cn}</code>. We want to find a matrix <code>P</code> that converts coordinates from basis B to basis C:<br><code>[v]_C = P_{C←B} * [v]_B</code></p><p>The columns of the <strong>change of basis matrix</strong> <code>P_{C←B}</code> are the coordinates of the basis vectors of B, expressed in terms of the basis C:<br><code>P_{C←B} = [[b1]_C  [b2]_C  ...  [bn]_C]</code></p><p>A common scenario is changing from a basis B to the standard basis E, or vice-versa. If E is the standard basis, <code>P_{E←B} = [b1 b2 ... bn]</code> (matrix with b's as columns), and <code>P_{B←E} = (P_{E←B})^{-1}</code>.</p><h3>Change of Basis for Linear Transformations:</h3><p>If A is the matrix of a transformation relative to basis B, its matrix D relative to basis C is:<br><code>D = P_{C←B} * A * P_{B←C}</code> where <code>P_{B←C} = (P_{C←B})^{-1}</code>.</p><h3>Takeaway:</h3><p>Changing basis allows us to view the same vector or transformation from different perspectives (coordinate systems). This is particularly useful for diagonalizing matrices (finding a basis where the transformation matrix is diagonal) and simplifying computations.</p>"
    },
     {
        "title": "Math: Orthogonality & Gram-Schmidt",
        "content": "<h3>Concept: Perpendicular Vectors and Bases</h3><p>In vector spaces, <strong>orthogonality</strong> generalizes the idea of perpendicularity. Two vectors are orthogonal if their dot product (or inner product) is zero. A set of vectors is orthogonal if every pair of distinct vectors in the set is orthogonal.</p><h3>Orthogonal Sets & Bases:</h3><ul><li>A set of non-zero orthogonal vectors is always linearly independent.</li><li>An <strong>orthogonal basis</strong> is a basis whose vectors are mutually orthogonal.</li><li>An <strong>orthonormal basis</strong> is an orthogonal basis where all vectors have unit length (magnitude 1). Orthonormal bases are particularly convenient because coordinate calculations become very simple.</li></ul><h3>The Gram-Schmidt Process:</h3><p>This is an algorithm for converting a set of linearly independent vectors (or a basis) into an orthogonal basis for the same subspace. If you further normalize each vector, you get an orthonormal basis.</p><h3>Steps (Orthogonalizing {v1, v2, ... vk}):</h3><ol><li>Let <code>u1 = v1</code>.</li><li>Let <code>u2 = v2 - proj_{u1}(v2)</code>, where <code>proj_u(v) = ((v · u) / (u · u)) * u</code> (the projection of v onto u).</li><li>Let <code>u3 = v3 - proj_{u1}(v3) - proj_{u2}(v3)</code>.</li><li>Continue this process: <code>ui = vi - Σ (proj_{uj}(vi))</code> for j = 1 to i-1.</li></ol><p>The resulting set <code>{u1, u2, ..., uk}</code> is an orthogonal set spanning the same subspace as <code>{v1, v2, ..., vk}</code>. To get an orthonormal basis, normalize each <code>ui</code>: <code>e_i = ui / ||ui||</code>.</p><h3>Takeaway:</h3><p>Orthogonal bases simplify many computations (like finding coordinates or projections). The Gram-Schmidt process is the standard method to construct an orthogonal (or orthonormal) basis from any given basis for a vector space, making these computations easier.</p>"
    },
    {
        "title": "Math: Systems of Linear ODEs (Matrix Method)",
        "content": "<h3>Concept: Solving Multiple Coupled Differential Equations</h3><p>Systems of linear ordinary differential equations (ODEs) describe how multiple interacting quantities change over time. When the equations are linear and have constant coefficients, matrix methods provide a powerful solution technique.</p><h3>Formulation:</h3><p>A system of n first-order linear ODEs with constant coefficients can be written in matrix form:<br><code>x'(t) = A * x(t) + f(t)</code></p><p>where:</p><ul><li><code>x(t)</code> is a vector function of time <code>t</code> (<code>x1(t), x2(t), ..., xn(t)</code>).</li><li><code>x'(t)</code> is the vector of derivatives (<code>x1'(t), ..., xn'(t)</code>).</li><li><code>A</code> is an <code>n x n</code> constant matrix of coefficients.</li><li><code>f(t)</code> is a vector function of time representing non-homogeneous terms.</li></ul><h3>Homogeneous Case (f(t) = 0):</h3><p><code>x'(t) = A * x(t)</code></p><p>Solutions are found using eigenvalues and eigenvectors of A. We guess solutions of the form <code>x(t) = e^(λt) * v</code>.</p><ul><li>Substituting gives <code>λ * e^(λt) * v = A * e^(λt) * v</code>. Since <code>e^(λt) ≠ 0</code>, this simplifies to <code>A * v = λ * v</code>.</li><li>This means <code>λ</code> must be an eigenvalue of A and <code>v</code> its corresponding eigenvector.</li><li>If A has n linearly independent eigenvectors <code>v1, ..., vn</code> with eigenvalues <code>λ1, ..., λn</code>, the general solution is a linear combination of these exponential solutions:<br><code>x(t) = c1 * e^(λ1*t) * v1 + c2 * e^(λ2*t) * v2 + ... + cn * e^(λn*t) * vn</code></li><li>Cases with complex eigenvalues or repeated eigenvalues require specific handling (involving complex exponentials, sines/cosines, or generalized eigenvectors).</li></ul><h3>Non-Homogeneous Case (f(t) ≠ 0):</h3><p>Solved by finding a particular solution (using methods like Variation of Parameters, similar to scalar ODEs, but using matrices) and adding it to the general solution of the homogeneous case.</p><h3>Takeaway:</h3><p>Matrix methods involving eigenvalues and eigenvectors are the standard approach for solving linear systems of ODEs with constant coefficients. The eigenvalues dictate the exponential growth/decay rates and oscillation frequencies, while eigenvectors determine the directions of these behaviors in the phase space.</p>"
    },
    {
        "title": "Math: Laplace Transforms (Solving ODEs)",
        "content": "<h3>Concept: Transforming ODEs to Algebra Problems</h3><p>The Laplace transform is a mathematical tool that converts a function of time <code>f(t)</code> (defined for t ≥ 0) into a function of a complex frequency <code>s</code>, denoted by <code>F(s) = L{f(t)}</code>. Its key utility in ODEs is that it transforms differentiation in the time domain into multiplication in the s-domain, turning a differential equation into an algebraic equation.</p><h3>Definition:</h3><p><code>F(s) = L{f(t)} = ∫[from 0 to ∞] e^(-st) * f(t) dt</code></p><h3>Key Property for ODEs:</h3><p>The transform of a derivative:<br><code>L{y'(t)} = s*Y(s) - y(0)</code><br><code>L{y''(t)} = s^2*Y(s) - s*y(0) - y'(0)</code><br>... and so on for higher derivatives, where <code>Y(s) = L{y(t)}</code> and <code>y(0), y'(0)</code> are initial conditions.</p><h3>Method for Solving Linear Constant-Coefficient ODEs:</h3><ol><li>Take the Laplace transform of both sides of the ODE. Use linearity and the derivative property, incorporating initial conditions.</li><li>The ODE is transformed into an algebraic equation in terms of <code>Y(s)</code> and <code>s</code>.</li><li>Solve this algebraic equation for <code>Y(s)</code>.</li><li>Find the <strong>inverse Laplace transform</strong> of <code>Y(s)</code> (<code>L^-1{Y(s)}</code>) to get the solution <code>y(t)</code> in the time domain. This often involves partial fraction decomposition of <code>Y(s)</code>.</li></ol><h3>Benefits:</h3><ul><li>Systematically handles initial conditions directly within the transformation.</li><li>Easily solves non-homogeneous equations, including those with discontinuous or impulse inputs (using transforms of step functions, delta functions).</li><li>Simplifies solving coupled systems of ODEs.</li></ul><h3>Takeaway:</h3><p>The Laplace transform provides a powerful, systematic method for solving linear ODEs with constant coefficients, especially initial value problems and those with tricky forcing functions. It elegantly converts the problem from the calculus domain to the algebra domain.</p>"
    },
    {
        "title": "Math: Fourier Series",
        "content": "<h3>Concept: Decomposing Periodic Functions</h3><p>A Fourier series represents a periodic function as a sum of sines and cosines (or complex exponentials) with different frequencies and amplitudes. It's analogous to expressing a musical chord as a combination of pure tones.</p><h3>Definition (for a periodic function f(x) with period 2L):</h3><p><code>f(x) = a0/2 + Σ[from n=1 to ∞] (an * cos(nπx/L) + bn * sin(nπx/L))</code></p><p>The coefficients <code>a0, an, bn</code> are calculated using integrals over one period, leveraging the orthogonality properties of sine and cosine functions:</p><ul><li><code>a0 = (1/L) * ∫[from -L to L] f(x) dx</code></li><li><code>an = (1/L) * ∫[from -L to L] f(x) * cos(nπx/L) dx</code></li><li><code>bn = (1/L) * ∫[from -L to L] f(x) * sin(nπx/L) dx</code></li></ul><h3>Convergence:</h3><p>For piecewise smooth functions (finite number of jumps and local extrema), the Fourier series converges to the function at points of continuity. At jump discontinuities, it converges to the average of the left and right limits.</p><h3>Applications:</h3><ul><li>Analyzing and solving differential equations (especially PDEs like the Heat and Wave equations) using method of Separation of Variables.</li><li>Signal processing (spectral analysis, filtering).</li><li>Image compression.</li><li>Analyzing vibrations and waves.</li></ul><h3>Complex Fourier Series:</h3><p>Using Euler's formula (<code>e^(ix) = cos(x) + i*sin(x)</code>), the series can be written in a more compact complex exponential form:<br><code>f(x) = Σ[from n=-∞ to ∞] cn * e^(inπx/L)</code><br>with <code>cn = (1/2L) * ∫[from -L to L] f(x) * e^(-inπx/L) dx</code>.</p><h3>Takeaway:</h3><p>Fourier series allow us to represent complex periodic functions as a sum of simple sinusoids. This 'frequency domain' representation is invaluable for analyzing periodic phenomena and is a cornerstone technique for solving many important partial differential equations.</p>"
    },
     {
        "title": "Math: Intro to Partial Differential Equations (PDEs)",
        "content": "<h3>Concept: Equations with Multiple Independent Variables</h3><p>A Partial Differential Equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives with respect to those variables. They are used to model physical phenomena involving spatial and temporal changes.</p><h3>Structure:</h3><p>Involves a function <code>u</code> that depends on multiple variables (e.g., <code>u(x, t)</code>, <code>u(x, y, z)</code>). The equation relates <code>u</code> and its partial derivatives (like <code>∂u/∂x</code>, <code>∂²u/∂t²</code>, etc.).</p><h3>Contrast with ODEs:</h3><ul><li>ODEs involve a single independent variable (e.g., <code>y(t)</code>). Solutions are typically functions of that single variable. Initial conditions at a single point are often sufficient.</li><li>PDEs involve multiple independent variables (e.g., <code>u(x, t)</code>). Solutions are functions of multiple variables. Requires both initial conditions (e.g., state at t=0) and boundary conditions (e.g., state at spatial edges).</li></ul><h3>Famous PDEs (Second Order Linear):</h3><ul><li><strong>Heat Equation:</strong> Models heat diffusion or other diffusion processes.<br><code>∂u/∂t = α * ∂²u/∂x²</code> (in 1D)</li><li><strong>Wave Equation:</strong> Models waves (vibrating strings, sound waves, electromagnetic waves).<br><code>∂²u/∂t² = c² * ∂²u/∂x²</code> (in 1D)</li><li><strong>Laplace's Equation:</strong> Models steady-state distributions (temperature in equilibrium, electric potential).<br><code>∂²u/∂x² + ∂²u/∂y² = 0</code> (in 2D)</li><li><strong>Poisson's Equation:</strong> A non-homogeneous version of Laplace's Equation.<br><code>∂²u/∂x² + ∂²u/∂y² = f(x, y)</code></li></ul><h3>Takeaway:</h3><p>PDEs are essential for describing complex physical systems where quantities vary across space and time. Solving them is generally harder than solving ODEs and requires specifying conditions not just at an initial point but also along spatial boundaries.</p>"
    },
    {
        "title": "Math: Solving PDEs (Separation of Variables)",
        "content": "<h3>Concept: Simplifying PDEs by Splitting Variables</h3><p>Separation of Variables is a common and powerful technique for solving certain linear partial differential equations, particularly homogeneous ones with homogeneous boundary conditions on simple domains.</p><h3>The Core Idea:</h3><p>Assume the solution <code>u(x, t, ...)</code> can be written as a product of functions, each depending on only one independent variable (e.g., <code>u(x, t) = X(x) * T(t)</code>).</p><h3>Method (Example: 1D Heat Equation ∂u/∂t = α ∂²u/∂x²):</h3><ol><li><strong>Assume Separated Form:</strong> Let <code>u(x, t) = X(x) * T(t)</code>.</li><li><strong>Substitute into PDE:</strong> Substitute this form into the PDE.<br><code>X(x) * T'(t) = α * X''(x) * T(t)</code></li><li><strong>Separate Variables:</strong> Rearrange the equation so that terms involving only <code>x</code> are on one side and terms involving only <code>t</code> are on the other.<br><code>T'(t) / (α * T(t)) = X''(x) / X(x)</code></li><li><strong>Introduce Separation Constant:</strong> Since the left side depends only on <code>t</code> and the right side only on <code>x</code>, and they are equal for all <code>x</code> and <code>t</code>, they must both be equal to a constant, say <code>λ</code> (the separation constant).<br><code>T'(t) / (α * T(t)) = λ</code> and <code>X''(x) / X(x) = λ</code></li><li><strong>Solve Two ODEs:</strong> This yields two independent ordinary differential equations:<br><code>T'(t) - αλ * T(t) = 0</code><br><code>X''(x) - λ * X(x) = 0</code></li><li><strong>Apply Boundary Conditions (to X(x)):</strong> Homogeneous boundary conditions (e.g., <code>u=0</code> at boundaries) typically lead to an eigenvalue problem for the spatial part <code>X(x)</code>, determining possible values of <code>λ</code> and corresponding non-trivial solutions <code>X_n(x)</code> (eigenfunctions).</li><li><strong>Solve Time ODE (for each λ_n):</strong> Solve the <code>T(t)</code> ODE for each valid <code>λ_n</code> to get solutions <code>T_n(t)</code>.</li><li><strong>Form General Solution:</strong> The principle of superposition states that any linear combination of these product solutions <code>u_n(x, t) = X_n(x) * T_n(t)</code> is also a solution. The general solution is an infinite series:<br><code>u(x, t) = Σ [from n=1 to ∞] An * X_n(x) * T_n(t)</code></li><li><strong>Apply Initial Conditions:</strong> Use the initial condition <code>u(x, 0) = f(x)</code> to find the coefficients <code>An</code>. This often involves using orthogonality of the eigenfunctions <code>X_n(x)</code> (related to Fourier series/Sturm-Liouville theory).</li></ol><h3>Takeaway:</h3><p>Separation of variables simplifies solving certain PDEs by breaking them down into simpler ODEs. It relies heavily on linear superposition and often involves solving eigenvalue problems and using techniques like Fourier series to satisfy initial conditions.</p>"
    },
    {
        "title": "Math: Phase Plane Analysis (Systems of ODEs)",
        "content": "<h3>Concept: Visualizing Solutions to Systems of ODEs</h3><p>For a system of two first-order autonomous ODEs (where the right-hand side does not explicitly depend on time), phase plane analysis is a graphical technique used to understand the behavior of solutions without explicitly solving the equations.</p><h3>Autonomous System:</h3><p><code>dx/dt = f(x, y)</code><br><code>dy/dt = g(x, y)</code></p><h3>The Phase Plane:</h3><p>A 2D plane where the x-axis represents the value of <code>x</code> and the y-axis represents the value of <code>y</code>. Each point <code>(x, y)</code> represents a state of the system.</p><h3>Phase Portrait:</h3><p>A plot of the phase plane showing typical trajectories (solution curves). At each point <code>(x, y)</code>, the vector <code>(f(x, y), g(x, y))</code> represents the velocity vector of a trajectory passing through that point (the direction and speed of change of <code>x</code> and <code>y</code>). Plotting these vectors or sketching solution curves creates the phase portrait.</p><h3>Equilibrium Points (Critical Points):</h3><ul><li>Points <code>(x*, y*)</code> where <code>f(x*, y*) = 0</code> and <code>g(x*, y*) = 0</code> simultaneously.</li><li>At these points, <code>dx/dt = 0</code> and <code>dy/dt = 0</code>, so the system state doesn't change. They represent constant solutions.</li><li>The behavior of trajectories near equilibrium points is crucial for understanding the system's long-term dynamics.</li></ul><h3>Stability Analysis (Linear Systems):</h3><p>For linear systems <code>x' = A * x</code>, the nature and stability of the equilibrium point at the origin <code>(0,0)</code> are determined by the eigenvalues of the matrix A (e.g., stable node, unstable node, saddle point, stable spiral, unstable spiral, center).</p><h3>Usefulness:</h3><ul><li>Provides qualitative understanding of solution behavior (tendency towards equilibrium, oscillations, unbounded growth).</li><li>Helps classify equilibrium points and their stability.</li><li>Visualizes system dynamics without complex formulas, especially when explicit solutions are difficult.</li></ul><h3>Takeaway:</h3><p>Phase plane analysis offers a geometric way to understand the behavior of 2D autonomous systems of ODEs by visualizing trajectories and the stability of equilibrium points. It's a powerful qualitative tool complementing analytical and numerical methods.</p>"
    },
    {
        "title": "Math: Existence & Uniqueness (Picard-Lindelöf)",
        "content": "<h3>Concept: Guaranteeing Solutions to ODEs</h3><p>When faced with a differential equation, we need to know if a solution exists and if it's the only possible solution that satisfies the given conditions. Existence and uniqueness theorems provide these guarantees under certain conditions.</p><h3>Initial Value Problem (IVP):</h3><p>Consider a first-order ODE:<br><code>y' = f(t, y)</code><br>with an initial condition:<br><code>y(t0) = y0</code></p><h3>Picard-Lindelöf Theorem (Existence & Uniqueness):</h3><p>If the function <code>f(t, y)</code> and its partial derivative with respect to y, <code>∂f/∂y</code>, are both <strong>continuous</strong> in a rectangular region <code>R</code> around the point <code>(t0, y0)</code>, then there exists a unique solution <code>y(t)</code> to the IVP in some interval <code>(t0 - h, t0 + h)</code> contained within <code>R</code>.</p><h3>Conditions:</h3><ul><li><strong>Continuity of f(t, y):</strong> Guarantees *existence* of a solution.</li><li><strong>Continuity of ∂f/∂y (Lipschitz Condition w.r.t. y):</strong> Guarantees *uniqueness* of the solution. If only f is continuous, a solution exists, but it might not be unique.</li></ul><h3>Intuition:</h3><p>Continuity of <code>f</code> ensures that the slope field <code>y'</code> doesn't have 'holes' or infinities that would prevent drawing a solution curve. Continuity of <code>∂f/∂y</code> (or the Lipschitz condition) ensures that the slope field doesn't behave too erratically. If <code>∂f/∂y</code> is bounded, it means slopes don't change infinitely fast as <code>y</code> changes, preventing solution curves from splitting apart like branches.</p><h3>Takeaway:</h3><p>The Picard-Lindelöf theorem provides crucial conditions (continuity of <code>f</code> and <code>∂f/∂y</code>) under which a first-order initial value problem is guaranteed to have one and only one solution in a local neighborhood of the initial point. This is foundational for the study and numerical analysis of ODEs.</p>"
    },
     {
        "title": "Math: Numerical Methods for ODEs",
        "content": "<h3>Concept: Approximating ODE Solutions</h3><p>Many ODEs cannot be solved analytically (i.e., in terms of elementary functions). Numerical methods provide ways to approximate the solution <code>y(t)</code> at discrete time points, given an initial value problem <code>y' = f(t, y), y(t0) = y0</code>.</p><h3>Core Idea:</h3><p>Start from the initial condition <code>(t0, y0)</code> and use the differential equation (which gives the slope <code>y'</code> at any point) to step forward in time, approximating the solution at subsequent points <code>t1, t2, ...</code>.</p><h3>Simple Method: Euler's Method:</h3><p>This is the most basic method. From point <code>(tn, yn)</code>, estimate the next point <code>(tn+1, yn+1)</code> using the slope at <code>(tn, yn)</code> and a step size <code>h = tn+1 - tn</code>:</p><p><code>yn+1 = yn + h * f(tn, yn)</code></p><ul><li>Geometrically, you're just following the tangent line segment over the interval <code>h</code>.</li><li>Simple, but generally inaccurate, especially for larger step sizes or functions with rapidly changing slopes. It has a local truncation error of O(h²) and a global error of O(h).</li></ul><h3>More Advanced Methods: Runge-Kutta Methods:</h3><p>These methods improve accuracy by evaluating the slope function <code>f(t, y)</code> at multiple points within the interval <code>h</code> and taking a weighted average of these slopes to get a better estimate of the average slope over the interval.</p><p>The most common is the Fourth-Order Runge-Kutta (RK4) method. It uses four slope evaluations (at the beginning, two intermediate points, and the end of the interval) to estimate the next step. RK4 has a local truncation error of O(h⁵) and a global error of O(h⁴), making it much more accurate than Euler's method for the same step size.</p><h3>Takeaway:</h3><p>Numerical methods are essential for solving ODEs that lack analytical solutions. They provide approximations by stepping through time using the slope field. Euler's method is basic, while Runge-Kutta methods (like RK4) offer significantly improved accuracy by using weighted averages of slopes within each step.</p>"
    },
    {
        "title": "Math: Sturm-Liouville Theory",
        "content": "<h3>Concept: Eigenvalue Problems for Second-Order ODEs</h3><p>Sturm-Liouville theory deals with a special type of second-order linear ODE, often arising from applying separation of variables to PDEs (like the Heat, Wave, or Laplace equations), and analyzes its eigenvalues and eigenfunctions subject to boundary conditions.</p><h3>Sturm-Liouville Equation (Regular Form):</h3><p><code>d/dx [ p(x) dy/dx ] + q(x) y + λ w(x) y = 0</code></p><p>on an interval <code>[a, b]</code>, subject to boundary conditions (e.g., <code>c1 y(a) + c2 y'(a) = 0</code>, <code>d1 y(b) + d2 y'(b) = 0</code>). Here, <code>y(x)</code> is the unknown function, <code>λ</code> is the eigenvalue, and <code>p(x), q(x), w(x)</code> are given functions (with <code>p(x) > 0</code>, <code>w(x) > 0</code>, and all typically real and continuous).</p><h3>Key Properties (for Regular SL Problems):</h3><ul><li>The eigenvalues <code>λn</code> are real and can be ordered: <code>λ1 < λ2 < λ3 < ...</code>, with <code>λn -> ∞</code> as <code>n -> ∞</code>.</li><li>For each eigenvalue <code>λn</code>, there is exactly one linearly independent corresponding eigenfunction <code>yn(x)</code> (up to a constant multiple).</li><li>Eigenfunctions corresponding to distinct eigenvalues are <strong>orthogonal</strong> with respect to the weight function <code>w(x)</code> on the interval <code>[a, b]</code>. The orthogonality relation is:<br><code>∫[from a to b] yn(x) * ym(x) * w(x) dx = 0</code> for <code>n ≠ m</code>.</li><li>The set of eigenfunctions <code>{y1(x), y2(x), ...}</code> forms a <strong>complete orthogonal basis</strong> for the space of square-integrable functions on <code>[a, b]</code> (with weight <code>w(x)</code>) that satisfy the boundary conditions.</li></ul><h3>Importance:</h3><p>The orthogonality and completeness of eigenfunctions are critical. They allow us to represent arbitrary functions (like initial conditions in PDEs) as a series expansion of these eigenfunctions, similar to how Fourier series represent functions using sines/cosines (which are eigenfunctions of a simple SL problem).</p><h3>Takeaway:</h3><p>Sturm-Liouville theory provides the framework for understanding the eigenvalues and eigenfunctions of second-order ODEs with boundary conditions. Its key result is the orthogonality and completeness of eigenfunctions, which forms the basis for using series expansions (like Fourier series and their generalizations) to solve boundary value problems and PDEs.</p>"
    },
     {
        "title": "Math: Green's Functions (ODEs/PDEs)",
        "content": "<h3>Concept: Solutions to 'Point Source' Problems</h3><p>A Green's function (G) for a linear differential operator (L) with boundary conditions is the solution to the homogeneous boundary value problem when the forcing term is a Dirac delta function (a 'point source' or 'impulse') at a specific point.</p><h3>Definition (for Linear Operator L and point ξ):</h3><p><code>L[G(x, ξ)] = δ(x - ξ)</code><br>where <code>δ(x - ξ)</code> is the Dirac delta function centered at <code>ξ</code>, and G satisfies the homogeneous boundary conditions.</p><h3>Why are they Useful?</h3><p>If you have the Green's function G(x, ξ) for an operator L and boundary conditions, you can find the solution <code>y(x)</code> to the non-homogeneous problem <code>L[y(x)] = f(x)</code> with the same boundary conditions by <strong>integrating</strong> the Green's function against the forcing term <code>f(x)</code>:</p><p><code>y(x) = ∫ G(x, ξ) * f(ξ) dξ</code> (integration over the domain of x)</p><p>This works because <code>f(x)</code> can be thought of as a sum of scaled delta functions, and by linearity, the total response is the sum (integral) of the responses to each point source.</p><h3>Finding Green's Functions:</h3><p>For ODEs, G typically satisfies the homogeneous equation for <code>x ≠ ξ</code>, has specific jump conditions at <code>x = ξ</code> related to the delta function, and satisfies the homogeneous boundary conditions.</p><p>For PDEs, the process is similar but involves solving a higher-dimensional problem. For linear time-invariant systems, the Green's function (or impulse response) is fundamental.</p><h3>Takeaway:</h3><p>Green's functions provide a systematic method for solving non-homogeneous linear differential equations (ODEs or PDEs) with boundary conditions. Once the Green's function (the response to a point source) is found, the solution for any arbitrary forcing function is obtained by a simple integration, making them powerful tools in physics and engineering.</p>"
    }
]
