[
    {
        "title": "Math: Linear Independence (Linear Algebra)",
        "content": "<h3>Concept: Non-Redundancy in Vectors</h3><p>A set of vectors <code>{v1, v2, ..., vk}</code> is <strong>linearly independent</strong> if the only way to form a linear combination of these vectors that results in the zero vector is when all the scalar coefficients are zero. It signifies that no vector in the set can be expressed as a combination of the others.</p><h3>Defining Equation & Trivial Solution:</h3><p>Consider the linear combination:<br><code>c1*v1 + c2*v2 + ... + ck*vk = 0</code> (where <code>0</code> is the zero vector)</p><p>This set of vectors is linearly independent <em>if and only if</em> the <em>only possible values</em> for the scalar coefficients (<code>c1, c2, ..., ck</code>) that satisfy this equation are:<br><code>c1 = 0, c2 = 0, ..., ck = 0</code></p><p>This unique solution where all coefficients are zero is called the <strong>trivial solution</strong>.</p><h3>Intuition & Geometric Interpretation:</h3><p>Think of the vectors as directions. If a set of vectors is linearly independent, adding any multiple of one vector will move you in a direction that *cannot* be achieved by only combining multiples of the other vectors. Each vector contributes uniquely to the set's span.</p><ul><li>In 2D: Two vectors are linearly independent if they are not parallel (one is not a scalar multiple of the other). They 'point' in different directions, allowing you to reach any point in the 2D plane (if they are non-zero).</li><li>In 3D: Three vectors are linearly independent if they don't all lie in the same plane passing through the origin.</li></ul><p>Each linearly independent vector adds a new dimension to the space they span.</p><h3>Contrast (Linear Dependence):</h3><p>If there exists *any* solution to the equation <code>c1*v1 + ... + ck*vk = 0</code> where <em>at least one</em> coefficient <code>ci</code> is <em>not</em> zero (a <strong>non-trivial solution</strong>), the set is <strong>linearly dependent</strong>.</p><p>If a set is linearly dependent, it means at least one vector <code>vi</code> in the set *can* be written as a linear combination of the *other* vectors. For example, if <code>c1*v1 + c2*v2 = 0</code> with <code>c1 ≠ 0</code>, then <code>v1 = (-c2/c1)*v2</code>, meaning <code>v1</code> is a scalar multiple of <code>v2</code> (they are collinear). This indicates redundancy in the set.</p><h3>Checking for Linear Independence:</h3><p>To check if a set of vectors is linearly independent, you can form a matrix where the columns (or rows) are the vectors. Then, solve the system of linear equations represented by <code>Ac = 0</code> (where A is the matrix and c is the vector of coefficients).</p><ul><li>The set is linearly independent if and only if the matrix A has <strong>full column rank</strong> (or its determinant is non-zero if it's a square matrix), which implies the system <code>Ac = 0</code> has only the trivial solution.</li><li>Alternatively, use Gaussian elimination to row reduce the matrix. The columns corresponding to pivot positions are linearly independent. If the number of pivot columns equals the number of vectors, they are linearly independent.</li></ul><h3>Takeaway:</h3><p>Linear independence is a core concept in Linear Algebra. It is fundamental for understanding the <strong>span</strong> of a set of vectors and for defining a <strong>basis</strong> – a minimal set of vectors that can generate an entire vector space. Linearly independent vectors are essential building blocks that are not redundant.</p>"
    },
    {
        "title": "CS: Array vs. Linked List",
        "content": "<h3>Concept: Linear Data Structures</h3><p>Two fundamental ways to store a sequence of elements.</p><h3>Array:</h3><ul><li>Stores elements in a <strong>contiguous block of memory</strong>.</li><li><strong>Access:</strong> O(1) - Direct access by index (<code>arr[i]</code>) is very fast.</li><li><strong>Insertion/Deletion (Middle):</strong> O(n) - Slow, requires shifting subsequent elements.</li><li><strong>Memory:</strong> Fixed size (often) or requires reallocation if dynamic.</li></ul><h3>Linked List:</h3><ul><li>Stores elements as <strong>nodes</strong>, each containing data and a pointer/reference to the next node. Nodes can be scattered in memory.</li><li><strong>Access:</strong> O(n) - Must traverse the list from the head to reach an element by position.</li><li><strong>Insertion/Deletion (Known Position/Node):</strong> O(1) - Fast, only requires updating pointers of adjacent nodes.</li><li><strong>Memory:</strong> Dynamic size, memory overhead per node (for the pointer).</li></ul><h3>Takeaway:</h3><p>Choose arrays for fast random access. Choose linked lists for frequent insertions/deletions in the middle of the sequence.</p>"
    },
    {
        "title": "CS: Hash Table Collisions",
        "content": "<h3>Concept: Handling Key Conflicts</h3><p>What happens when different keys hash to the same index in a hash table?</p><h3>Ideal Hash Table:</h3><ul><li>Average O(1) for Insert, Search, Delete.</li><li>Relies on a good hash function distributing keys evenly.</li></ul><h3>Collision:</h3><ul><li>Occurs when <code>hash(key1) == hash(key2)</code> for <code>key1 != key2</code>.</li><li>Must have a strategy to store both elements.</li></ul><h3>Common Strategies:</h3><ul><li><strong>Chaining:</strong> Each array slot holds a pointer to a linked list (or other structure) of all elements that hash to that index.<ul><li><strong>Pros:</strong> Simple to implement, deletion is easy.</li><li><strong>Cons:</strong> Requires extra space for pointers, cache performance can be poor traversing lists.</li></ul></li><li><strong>Open Addressing:</strong> If a slot is occupied, probe for the next available slot using a predefined sequence (linear, quadratic probing, double hashing). Element is stored *directly* in the table.<ul><li><strong>Pros:</strong> Better cache performance, no extra space for pointers per element.</li><li><strong>Cons: Sandwiched</strong> (deletion is complex and can break search chains), can suffer from clustering.</li></ul></li></ul><h3>Takeaway:</h3><p>Collision resolution is key to Hash Table performance. Chaining is generally simpler, while Open Addressing can offer better cache locality but requires more complex management, especially for deletions.</p>"
    },
    {
        "title": "CS: BST vs. Balanced BST",
        "content": "<h3>Concept: Tree Search Performance</h3><p>Organizing data hierarchically for efficient lookups, and guaranteeing performance.</p><h3>Binary Search Tree (BST):</h3><ul><li>Left child < Node < Right child.</li><li><strong>Operations (Average):</strong> O(log n) for Search, Insert, Delete.</li><li><strong>Operations (Worst Case):</strong> O(n) - Occurs when tree becomes skewed (like a linked list, e.g., inserting sorted data).</li><li>Simple structure and operations.</li></ul><h3>Balanced BST (e.g., AVL, Red-Black Tree):</h3><ul><li>Maintain a balanced height using rotations during Insert/Delete operations.</li><li>Examples: AVL Trees (strictly balanced height), Red-Black Trees (properties guarantee balance).</li><li><strong>Operations (Guaranteed):</strong> O(log n) for Search, Insert, Delete.</li><li>More complex implementation due to rebalancing logic.</li></ul><h3>Takeaway:</h3><p>Use a standard BST when average-case O(log n) is sufficient and implementation simplicity is prioritized. Use a Balanced BST (like Red-Black Trees, commonly used in standard library maps/sets) when worst-case O(log n) performance *guarantees* are required, accepting the overhead of rebalancing operations.</p>"
    },
    {
        "title": "CS: Graph Representations",
        "content": "<h3>Concept: Storing Graph Structure</h3><p>Two primary ways to represent connections (edges) between nodes (vertices) in memory.</p><h3>Adjacency Matrix:</h3><ul><li>A V x V 2D array (or matrix), where V is the number of vertices.</li><li><code>adj[i][j] = 1</code> (or weight) if an edge exists from vertex i to vertex j, otherwise 0 (or infinity).</li><li><strong>Space:</strong> O(V^2).</li><li><strong>Check Edge (u, v):</strong> O(1) - Direct lookup.</li><li><strong>Find Neighbors of u:</strong> O(V) - Must iterate through the row for u.</li><li>Good for <strong>Dense Graphs</strong> (many edges).</li></ul><h3>Adjacency List:</h3><ul><li>An array or map where each index/key u stores a list (or set) of vertices adjacent to u.</li><li><code>adj[u] = [v1, v2, ...]</code> where (u, v1), (u, v2) are edges.</li><li><strong>Space:</strong> O(V + E), where E is the number of edges.</li><li><strong>Check Edge (u, v):</strong> O(deg(u)) in worst case for simple list, O(log V) or O(1) on average if using sorted lists or hash sets. Let's assume O(deg(u)) for a simple list.</li><li><strong>Find Neighbors of u:</strong> O(deg(u)) - Iterate through the list for u.</li><li>Good for <strong>Sparse Graphs</strong> (few edges).</li></ul><h3>Takeaway:</h3><p>Choose Adjacency Matrix for dense graphs or when frequent O(1) edge lookups are critical. Choose Adjacency List for sparse graphs or when iterating through vertex neighbors is frequent, as it's more space-efficient for sparse graphs.</p>"
    },
     {
        "title": "CS: Processes vs. Threads",
        "content": "<h3>Concept: Units of Execution</h3><p>How operating systems manage running tasks and their resources.</p><h3>Process:</h3><ul><li>An instance of a program executing (e.g., a running web browser).</li><li>Has its <strong>own dedicated memory space</strong>, file handles, stack, heap, etc. Independent resources.</li><li><strong>Heavyweight:</strong> Slower to create and context-switch between processes due to separate memory spaces.</li><li><strong>Communication:</strong> Requires Inter-Process Communication (IPC) mechanisms (pipes, sockets, shared memory) - slower but safer.</li><li>Provides isolation: If one process crashes, it doesn't directly affect others.</li></ul><h3>Thread:</h3><ul><li>A single sequence of execution *within* a process (e.g., different tabs in a browser often run in separate threads).</li><li>Shares the <strong>same memory space</strong> (heap, global variables) and resources (file handles) as other threads within the same process. Has its own stack and registers.</li><li><strong>Lightweight:</strong> Faster to create and context-switch between threads.</li><li><strong>Communication:</strong> Easy via shared memory - faster but requires careful synchronization (locks, mutexes) to avoid race conditions.</li><li>Lack of isolation: If one thread crashes, it can potentially crash the entire process.</li></ul><h3>Takeaway:</h3><p>Use Processes for strong isolation and fault tolerance (e.g., running separate applications). Use Threads for concurrency within a single application where fast communication and shared data are needed, but be mindful of synchronization challenges.</p>"
    },
    {
        "title": "CS: Binary Heaps",
        "content": "<h3>Concept: Priority Queues</h3><p>An efficient data structure to manage items with priorities, allowing fast access to the highest (or lowest) priority item.</p><h3>Binary Heap:</h3><ul><li>A <strong>complete binary tree</strong> (all levels full except possibly the last, which is filled left-to-right).</li><li>Satisfies the <strong>Heap Property</strong>:</li>    <ul><li><strong>Min-Heap:</strong> Parent node key is less than or equal to the keys of its children. The minimum element is at the root.</li><li><strong>Max-Heap:</strong> Parent node key is greater than or equal to the keys of its children. The maximum element is at the root.</li></ul><li>Often implemented using an <strong>array</strong>, leveraging the complete tree property for implicit node relationships.</li><li><strong>Operations:</strong></li>    <ul><li><strong>Insert:</strong> O(log n) - Add to end, bubble up.</li><li><strong>Extract-Min/Max:</strong> O(log n) - Remove root, replace with last element, bubble down.</li><li><strong>Peek-Min/Max:</strong> O(1) - Look at root.</li></ul></ul><h3>Takeaway:</h3><p>Binary heaps are ideal for implementing Priority Queues, offering efficient O(log n) operations for inserting and extracting the highest/lowest priority item, and O(1) access to the top element.</p>"
    },
    {
        "title": "CS: Tries (Prefix Trees)",
        "content": "<h3>Concept: Efficient String Storage & Retrieval</h3><p>A tree-like data structure specifically for storing dynamic sets or associative arrays where keys are strings.</p><h3>Trie Structure:</h3><ul><li>Nodes represent prefixes. Each node has children corresponding to the next possible characters in a key.</li><li>The root represents the empty string.</li><li>Edges are labeled with characters.</li><li>Paths from the root to certain nodes represent keys (often marked with a special 'end of word' flag).</li><li>Nodes can store associated values.</li></ul><h3>Performance:</h3><ul><li><strong>Search/Insert/Delete:</strong> O(L), where L is the length of the key. This is faster than O(L log N) for balanced BSTs or O(L) average for hash tables (due to hashing overhead, especially with long keys), and eliminates collisions.</li><li><strong>Space:</strong> Can be significant in worst case (e.g., storing all permutations), but often efficient when keys share prefixes.</li><li><strong>Prefix Search:</strong> Tries excel at finding all keys with a given prefix.</li></ul><h3>Takeaway:</h3><p>Tries are excellent for dictionary-like applications involving strings (autocomplete, spell checking, IP routing) where fast prefix-based operations and search time independent of the number of keys are crucial.</p>"
    },
    {
        "title": "CS: Skip Lists",
        "content": "<h3>Concept: Probabilistic Search Structure</h3><p>A data structure that allows O(log n) average time complexity for search, insertion, and deletion operations, built on top of sorted linked lists.</p><h3>Structure:</h3><ul><li>Consists of multiple layers of sorted linked lists.</li><li>The bottom layer is a simple sorted linked list containing all elements.</li><li>Each subsequent layer is a 'subsequence' of the layer below, containing a randomly chosen subset of nodes (often using a coin flip).</li><li>Higher layers have fewer nodes and allow for 'skipping' over large numbers of nodes in lower layers during traversal.</li></ul><h3>Operations:</h3><ul><li><strong>Search:</strong> Start at the top-left node, traverse right as long as the current node's value is less than the target value. If the next node is too large or null, move down one layer. Repeat until the bottom layer is searched.</li><li><strong>Insertion/Deletion:</strong> Search to find the position in the bottom layer. Insert/delete the node. Then, probabilistically 'promote' or 'remove' the node from higher layers based on the original probabilistic decision.</li></ul><h3>Takeaway:</h3><p>Skip lists offer competitive average-case performance (O(log n)) compared to balanced trees, are generally simpler to implement, and provide good concurrent access properties. Their performance guarantee is probabilistic rather than deterministic.</p>"
    },
    {
        "title": "CS: B-Trees (and B+ Trees)",
        "content": "<h3>Concept: Efficient Disk-Based Data Structures</h3><p>Trees optimized for storage systems that read and write large blocks of data (like hard drives or SSDs).</p><h3>B-Tree:</h3><ul><li>A self-balancing M-ary tree (M > 2, typically large).</li><li>Nodes can have many children (high 'fan-out').</li><li>Data records are stored in internal and leaf nodes.</li><li>Designed so that related nodes (children of a node) are likely in the same disk block, minimizing disk seeks.</li><li><strong>Search/Insert/Delete:</strong> O(log_M n). The large M means the tree is very shallow, minimizing disk block reads.</li></ul><h3>B+ Tree:</h3><ul><li>A variation commonly used in databases and file systems.</li><li><strong>All data records are stored only in the leaf nodes.</strong></li><li>Leaf nodes are linked together in a sorted list, allowing for efficient range queries.</li><li>Internal nodes only store keys (and pointers), serving as an index.</li><li>Even better for disk reads as internal nodes can pack more keys/pointers per block.</li></ul><h3>Takeaway:</h3><p>B-Trees and especially B+ Trees are fundamental for systems dealing with large datasets on disk, like databases. Their structure minimizes expensive disk I/O operations by maximizing the data/pointers stored in each block read.</p>"
    },
    {
        "title": "CS: Sorting Algorithms (QuickSort vs MergeSort)",
        "content": "<h3>Concept: Efficiently Ordering Data</h3><p>Comparing two popular O(n log n) sorting algorithms.</p><h3>QuickSort:</h3><ul><li><strong>Divide and Conquer:</strong> Picks a 'pivot', partitions the array into elements less than/greater than the pivot, recursively sorts the partitions.</li><li><strong>Average Case:</strong> O(n log n).</li><li><strong>Worst Case:</strong> O(n^2) (occurs with poor pivot choices, e.g., already sorted array).</li><li><strong>Space:</strong> O(log n) average stack space (recursive calls), O(n) worst case. Can be O(1) in-place with careful implementation.</li><li><strong>Cache Performance:</strong> Generally good due to locality of reference within partitions.</li><li>Typically faster in practice than MergeSort due to lower overhead and better cache performance.</li></ul><h3>MergeSort:</h3><ul><li><strong>Divide and Conquer:</strong> Divides array in half, recursively sorts halves, then merges sorted halves.</li><li><strong>Worst Case:</strong> O(n log n) - Performance is consistent regardless of input.</li><li><strong>Space:</strong> O(n) auxiliary space needed for merging.</li><li><strong>Stability:</strong> Is a stable sort (maintains relative order of equal elements).</li><li>Good for sorting linked lists and external sorting.</li></ul><h3>Takeaway:</h3><p>QuickSort is often preferred for arrays due to its typical speed and potential for in-place sorting, though its worst-case performance is a concern. MergeSort is chosen when stable sorting or guaranteed O(n log n) performance regardless of input is required, or for external sorting.</p>"
    },
    {
        "title": "CS: Dynamic Programming",
        "content": "<h3>Concept: Solving Complex Problems via Subproblems</h3><p>An algorithmic technique for solving optimization problems by breaking them into simpler subproblems and storing their results to avoid redundant computations.</p><h3>Core Principles:</h3><ul><li><strong>Optimal Substructure:</strong> An optimal solution to the problem contains optimal solutions to subproblems.</li><li><strong>Overlapping Subproblems:</strong> The same subproblems are encountered multiple times when solving the problem recursively.</li></ul><h3>Approaches:</h3><ul><li><strong>Memoization (Top-Down):</strong> Start from the main problem, recursively solve subproblems, store results in a cache (e.g., array, hash map) when computed for the first time, and return cached result if available. Natural recursive structure.</li><li><strong>Tabulation (Bottom-Up):</strong> Start with the smallest subproblems, compute their solutions, and store them in a table. Use table values to compute solutions for larger subproblems until the main problem's solution is reached. Typically iterative.</li></ul><h3>Takeaway:</h3><p>Dynamic Programming is applicable when a problem has optimal substructure and overlapping subproblems. It transforms exponential-time naive recursive solutions into polynomial-time solutions by systematically solving and storing results of subproblems.</p>"
    },
    {
        "title": "CS: Greedy Algorithms",
        "content": "<h3>Concept: Local Optimality</h3><p>An algorithmic paradigm that makes the locally optimal choice at each step with the hope of finding a global optimum.</p><h3>Characteristics:</h3><ul><li>Makes a choice that seems best at the moment, without regard for future consequences.</li><li>Does not reconsider previous choices.</li><li>Often simpler and faster than dynamic programming or other approaches.</li></ul><h3>Applicability:</h3><ul><li>Greedy algorithms *do not* always produce a globally optimal solution.</li><li>They work for specific problems that exhibit the 'greedy choice property' (a locally optimal choice extends to a global optimum) and optimal substructure.</li></ul><h3>Examples where Greedy Works:</h3><ul><li>Dijkstra's Shortest Path Algorithm (with non-negative weights)</li><li>Prim's and Kruskal's Minimum Spanning Tree algorithms</li><li>Huffman Coding</li><li>Activity Selection Problem</li></ul><h3>Examples where Greedy Fails:</h3><ul><li>Coin Change Problem (depending on coin denominations) - DP is needed for optimal solution.</li><li>Travelling Salesperson Problem</li></ul><h3>Takeaway:</h3><p>Greedy algorithms are simple and fast but are only suitable for problems where the greedy choice property holds. Verifying this property is crucial before applying a greedy approach.</p>"
    },
    {
        "title": "CS: Backtracking",
        "content": "<h3>Concept: Exploring Possibilities Systematically</h3><p>An algorithmic technique for finding solutions to computational problems, particularly constraint satisfaction problems, by trying to build a solution incrementally and 'backtracking' (undoing the last step) when a partial solution cannot be extended to a complete solution.</p><h3>Process:</h3><ul><li>Build a solution step by step (e.g., assigning a value to a variable, placing an item).</li><li>At each step, check if the current partial solution violates any constraints.</li><li>If a constraint is violated, abandon this path and backtrack to the previous step to try a different choice.</li><li>If a partial solution is valid, recursively proceed to the next step.</li><li>If a complete, valid solution is found, record it (or stop).</li></ul><h3>Analogy:</h3><p>Exploring a maze. You follow a path until you hit a dead end, then go back to the last junction and try another path.</p><h3>Use Cases:</h3><ul><li>Solving puzzles (Sudoku, N-Queens)</li><li>Combinatorial problems (finding subsets, permutations, combinations)</li><li>Constraint satisfaction problems</li><li>Parsing</li></ul><h3>Takeaway:</h3><p>Backtracking is a form of depth-first search on the state-space tree of the problem. It is effective for searching for solutions in a large solution space by pruning branches that cannot lead to a valid answer, though it can still be exponential in the worst case.</p>"
    },
    {
        "title": "CS: BFS vs. DFS (Graph Traversal)",
        "content": "<h3>Concept: Exploring Graphs/Trees</h3><p>Two fundamental algorithms for visiting all nodes in a graph or tree.</p><h3>Breadth-First Search (BFS):</h3><ul><li>Explores level by level. Visits all neighbors of a node before moving to the next level.</li><li>Uses a <strong>Queue</strong> to manage nodes to visit.</li><li>Guaranteed to find the <strong>shortest path in terms of number of edges</strong> in an unweighted graph.</li><li>Can require more memory than DFS (O(V) in worst case for queue).</li><li>Typically implemented iteratively.</li></ul><h3>Depth-First Search (DFS):</h3><ul><li>Explores as deeply as possible along each branch before backtracking.</li><li>Uses a <strong>Stack</strong> (explicit or via recursion call stack) to manage nodes to visit.</li><li>Useful for detecting cycles, topological sorting, and exploring connected components.</li><li>Memory usage is O(height of tree/graph) in recursion stack, O(V+E) for explicit stack + visited set.</li><li>Can be implemented recursively or iteratively.</li></ul><h3>Takeaway:</h3><p>Use BFS for finding shortest paths (edge count) or when you need to explore nodes level by level. Use DFS for exploring the full depth of a branch, detecting cycles, or when memory is constrained (in deep graphs/trees).</p>"
    },
    {
        "title": "CS: Dijkstra vs. Bellman-Ford (Shortest Path)",
        "content": "<h3>Concept: Finding Minimum Cost Paths</h3><p>Two classic algorithms for finding the shortest paths from a single source vertex to all other vertices in a graph.</p><h3>Dijkstra's Algorithm:</h3><ul><li>Finds shortest paths in graphs with <strong>non-negative edge weights</strong>.</li><li>A greedy algorithm. Maintains a set of visited nodes and a priority queue of unvisited nodes, always selecting the unvisited node with the current smallest known distance.</li><li><strong>Time Complexity:</strong> O(E log V) with a binary heap, or O(E + V log V) with a Fibonacci heap.</li><li>Does <strong>not</strong> work correctly with negative edge weights.</li></ul><h3>Bellman-Ford Algorithm:</h3><ul><li>Finds shortest paths in graphs that may contain <strong>negative edge weights</strong>.</li><li>Relaxes all edges V-1 times. If relaxing edges the V-th time still reduces a path length, a <strong>negative cycle</strong> exists (and shortest paths are undefined).</li><li><strong>Time Complexity:</strong> O(V * E). Slower than Dijkstra for graphs with non-negative weights.</li><li>Can detect negative cycles reachable from the source.</li></ul><h3>Takeaway:</h3><p>Use Dijkstra's Algorithm for faster shortest path calculations when all edge weights are non-negative. Use Bellman-Ford when the graph may contain negative edge weights, accepting the higher time complexity, and also to detect negative cycles.</p>"
    },
     {
        "title": "CS: Prim vs. Kruskal (MST)",
        "content": "<h3>Concept: Finding Minimum Spanning Trees</h3><p>Two greedy algorithms to find a Minimum Spanning Tree (MST) in a connected, undirected graph with weighted edges.</p><h3>Minimum Spanning Tree (MST):</h3><p>A subset of the edges that connects all the vertices together, without any cycles, and with the minimum possible total edge weight.</p><h3>Prim's Algorithm:</h3><ul><li>Starts with an arbitrary vertex and grows the MST by adding the minimum-weight edge connecting a vertex in the current MST to a vertex outside the MST.</li><li>Uses a <strong>priority queue</strong> to efficiently find the next minimum-weight edge.</li><li>Often implemented on an adjacency list representation.</li><li><strong>Time Complexity:</strong> O(E log V) or O(E + V log V) with a Fibonacci heap.</li></ul><h3>Kruskal's Algorithm:</h3><ul><li>Starts with an empty MST and adds edges in increasing order of weight, skipping edges that would form a cycle with already added edges.</li><li>Uses a <strong>Disjoint Set Union (DSU)</strong> data structure to efficiently check if adding an edge creates a cycle and to merge connected components.</li><li>Often implemented by sorting edges first.</li><li><strong>Time Complexity:</strong> O(E log E) or O(E log V) (since E is at most V^2, log E is O(log V^2) = O(log V), or more precisely, sorting dominates at O(E log E)).</li></ul><h3>Takeaway:</h3><p>Both Prim's and Kruskal's find an MST. Prim's expands from a vertex, good for dense graphs or when a starting vertex is specified. Kruskal's adds edges globally based on weight, often simpler to understand and implement, and efficient for sparse graphs.</p>"
    },
    {
        "title": "CS: Recursion vs. Iteration",
        "content": "<h3>Concept: Repeated Execution</h3><p>Two fundamental programming approaches for performing repetitive tasks.</p><h3>Recursion:</h3><ul><li>A function calls itself, breaking a problem down into smaller, self-similar subproblems.</li><li>Requires a <strong>base case</strong> to stop the recursion.</li><li>Uses the <strong>call stack</strong> to manage state (parameters, local variables) for each recursive call.</li><li>Can be elegant and mirrors mathematical definitions for problems with recursive structure (e.g., factorial, tree traversals).</li><li>Risk of <strong>Stack Overflow</strong> for deep recursion if not optimized (Tail Call Optimization).</li><li>Overhead due to function calls and stack management.</li></ul><h3>Iteration:</h3><ul><li>Uses loops (<code>for</code>, <code>while</code>) to repeat a block of code.</li><li>State is managed explicitly using loop variables and data structures.</li><li>Generally uses less memory than recursion (no function call stack per iteration).</li><li>Often more intuitive for simple repetitions.</li><li>Can be less readable for problems with deeply nested or complex recursive structure.</li></ul><h3>Takeaway:</h3><p>Choose Recursion for problems with an inherent recursive structure (trees, graphs, divide-and-conquer) where it leads to cleaner code, but be mindful of stack limits. Choose Iteration for simple repetitions or when performance/memory usage is critical and the problem can be naturally expressed with loops.</p>"
    },
     {
        "title": "CS: Virtual Memory",
        "content": "<h3>Concept: Illusion of Infinite Memory</h3><p>An OS technique that provides processes with the illusion of having a large, contiguous block of memory, even if physical RAM is fragmented or limited, by using disk space as an extension of RAM.</p><h3>How it Works:</h3><ul><li>Divides a process's logical address space and physical memory into fixed-size chunks (<strong>pages</strong> for logical, <strong>frames</strong> for physical).</li><li>Maintains a <strong>Page Table</strong> for each process, mapping logical page numbers to physical frame numbers.</li><li>Allows parts of a process to reside in RAM while others are temporarily stored on disk (<strong>swapping</strong> or <strong>paging</strong>).</li><li>When a process accesses a page not in RAM, a <strong>Page Fault</strong> occurs. The OS loads the required page from disk into a free frame in RAM (possibly evicting another page using a page replacement algorithm like LRU).</li></ul><h3>Benefits:</h3><ul><li>Allows processes larger than physical memory to run.</li><li>Enables memory protection and isolation between processes.</li><li>Simplifies memory management for programmers.</li><li>Facilitates efficient memory sharing between processes.</li></ul><h3>Takeaway:</h3><p>Virtual Memory is a cornerstone of modern OS, providing memory isolation, allowing multitasking with limited RAM, and enabling programs larger than physical memory. It relies heavily on hardware support (MMU) and comes with potential performance costs (thrashing) if excessive swapping occurs.</p>"
    },
    {
        "title": "CS: Concurrency Issues (Deadlock vs. Race Condition)",
        "content": "<h3>Concept: Problems in Parallel Execution</h3><p>Common pitfalls when multiple processes or threads access shared resources concurrently.</p><h3>Race Condition:</h3><ul><li>Occurs when the output of a concurrent program depends on the unpredictable order in which operations by multiple processes/threads are interleaved.</li><li>Results from multiple entities accessing and modifying shared data without proper synchronization.</li><li>The 'race' is between processes/threads to access/modify the shared resource first.</li><li>Can lead to corrupted data or incorrect program behavior.</li><li><strong>Mitigation:</strong> Use synchronization primitives like mutexes, semaphores, locks, or atomic operations to ensure controlled access to shared resources.</li></ul><h3>Deadlock:</h3><ul><li>A state where two or more processes are blocked indefinitely, waiting for each other to release resources that they hold.</li><li>Requires four necessary conditions (Coffman Conditions) to occur simultaneously: <strong>Mutual Exclusion, Hold and Wait, No Preemption, Circular Wait</strong>.</li><li>Processes cannot proceed because each is waiting for a resource held by another process in the cycle.</li><li><strong>Mitigation:</strong> Prevention (negate one of the four conditions), Avoidance (e.g., Banker's algorithm), Detection (find cycles in resource allocation graph) & Recovery (preempt resources, terminate processes).</li></ul><h3>Takeaway:</h3><p>Race conditions are about timing-dependent data corruption due to unsynchronized access to shared data. Deadlocks are about processes being stuck indefinitely, waiting for resources held by others. Both require careful design and synchronization in concurrent systems.</p>"
    },
    {
        "title": "CS: ACID Properties (Databases)",
        "content": "<h3>Concept: Transaction Reliability</h3><p>A set of properties guaranteeing that database transactions are processed reliably.</p><h3>Transaction:</h3><p>A single unit of work that must be completed entirely or not at all.</p><h3>ACID Properties:</h3><ul><li><strong>Atomicity:</strong> A transaction is treated as a single, indivisible unit. Either all operations within it are completed successfully, or none of them are (it's rolled back). Prevents partial updates.</li><li><strong>Consistency:</strong> A transaction brings the database from one valid state to another valid state. It must not violate any database invariants (rules, constraints).</li><li><strong>Isolation:</strong> Concurrent transactions do not interfere with each other. Each transaction appears to run as if it is the only transaction operating on the database, even if others are executing concurrently. Achieved via locking or multiversion concurrency control.</li><li><strong>Durability:</strong> Once a transaction is committed, its changes are permanent and survive system failures (power loss, crashes). Typically achieved by writing changes to persistent storage (disk, transaction logs).</li></ul><h3>Takeaway:</h3><p>Adhering to ACID properties is fundamental for building reliable and robust database systems, ensuring data integrity and consistency even in the presence of concurrent access and failures. They provide a strong guarantee about the outcome of transactions.</p>"
    },
    {
        "title": "CS: Database Indexing",
        "content": "<h3>Concept: Speeding up Data Retrieval</h3><p>A data structure technique used to quickly locate and access data within a database table without having to scan the entire table.</p><h3>How it Works:</h3><ul><li>An index is typically a separate data structure (most commonly a <strong>B+ Tree</strong>) that stores a small subset of the data from a table column(s) along with pointers to the full data rows.</li><li>When you query the database based on an indexed column, the database system first searches the index (which is fast due to its structure and smaller size) to find the location of the relevant rows, then retrieves the full rows directly.</li></ul><h3>Benefits:</h3><ul><li>Significantly speeds up read operations (<code>SELECT</code>) for queries using indexed columns in <code>WHERE</code> clauses, <code>JOIN</code> conditions, or <code>ORDER BY</code> clauses.</li></ul><h3>Drawbacks:</h3><ul><li>Requires extra storage space for the index itself.</li><li>Slows down write operations (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) because the index must also be updated whenever the underlying data changes.</li><li>Indexes need to be chosen carefully based on query patterns; too many indexes can hurt write performance without providing commensurate read benefits.</li></ul><h3>Takeaway:</h3><p>Indexing is a crucial database optimization technique for improving read performance, especially on large tables. It involves a trade-off between read speed and write speed/storage space and should be applied judiciously to frequently queried columns.</p>"
    },
    {
        "title": "CS: TCP vs. UDP",
        "content": "<h3>Concept: Transport Layer Protocols</h3><p>Two core protocols in the internet protocol suite (part of the Transport Layer) for sending data between applications.</p><h3>Transmission Control Protocol (TCP):</h3><ul><li><strong>Connection-Oriented:</strong> Establishes a connection (handshake) before sending data.</li><li><strong>Reliable:</strong> Guarantees delivery of data, retransmits lost packets, orders packets correctly. Uses acknowledgments and sequence numbers.</li><li><strong>Flow Control:</strong> Prevents sender from overwhelming receiver.</li><li><strong>Congestion Control:</strong> Manages network traffic to avoid collapse.</li><li><strong>Header Overhead:</strong> Larger header due to more features.</li><li><strong>Use Cases:</strong> Web browsing (HTTP/S), Email (SMTP), File Transfer (FTP) - where reliability is critical.</li></ul><h3>User Datagram Protocol (UDP):</h3><ul><li><strong>Connectionless:</strong> Sends data packets (datagrams) without establishing a connection. 'Fire and forget'.</li><li><strong>Unreliable:</strong> Does not guarantee delivery, order, or duplicate prevention.</li><li><strong>No Flow/Congestion Control:</strong> Sends data as fast as the application provides it.</li><li><strong>Header Overhead:</strong> Minimal header.</li><li><strong>Speed:</strong> Faster due to less overhead and no waiting for acknowledgments.</li><li><strong>Use Cases:</strong> Streaming (audio/video), Online Gaming, DNS, VoIP - where speed and low latency are prioritized over guaranteed delivery of every single packet.</li></ul><h3>Takeaway:</h3><p>Choose TCP when reliable, ordered delivery is paramount, accepting the overhead. Choose UDP when speed and low latency are more important than guaranteed delivery, and the application can handle potential packet loss or reordering.</p>"
    },
    {
        "title": "CS: Compilation vs. Interpretation",
        "content": "<h3>Concept: Executing Code</h3><p>Two primary approaches for translating and running source code.</p><h3>Compilation:</h3><ul><li>Source code is translated into machine code (or intermediate bytecode) <strong>once</strong> by a compiler before execution.</li><li>The output is an executable file that can be run directly by the operating system/hardware.</li><li><strong>Process:</strong> Source Code -> Compiler -> Object Code -> Linker -> Executable.</li><li><strong>Execution Speed:</strong> Generally faster because the translation is done upfront.</li><li><strong>Development Cycle:</strong> Slower edit-compile-run cycle.</li><li><strong>Error Detection:</strong> Errors (like syntax errors) are typically caught during compilation.</li><li><strong>Examples:</strong> C, C++, Java (compiled to bytecode), Go.</li></ul><h3>Interpretation:</h3><ul><li>Source code is translated and executed <strong>line by line</strong> or statement by statement by an interpreter at runtime.</li><li>No separate executable file is produced.</li><li><strong>Process:</strong> Source Code -> Interpreter -> Execution.</li><li><strong>Execution Speed:</strong> Generally slower due to the runtime translation overhead.</li><li><strong>Development Cycle:</strong> Faster edit-run cycle.</li><li><strong>Error Detection:</strong> Errors may only be found when the corresponding line of code is executed.</li><li><strong>Examples:</strong> Python, JavaScript, Ruby, PHP.</li></ul><h3>Hybrid Approaches (JIT):</h3><p>Some languages/runtimes use Just-In-Time (JIT) compilation, compiling code during execution for performance benefits (e.g., Java JVM, V8 for JavaScript).</p><h3>Takeaway:</h3><p>Compilation favors execution speed and early error detection, suitable for performance-critical applications. Interpretation favors faster development cycles and flexibility, suitable for scripting and rapid prototyping. Hybrid approaches aim to combine the benefits.</p>"
    },
    {
        "title": "CS: Garbage Collection",
        "content": "<h3>Concept: Automatic Memory Management</h3><p>A form of automatic memory management that attempts to reclaim memory occupied by objects that are no longer reachable or used by the program.</p><h3>Manual Memory Management:</h3><ul><li>Programmer is responsible for allocating and deallocating memory (e.g., <code>malloc</code>/<code>free</code> in C, <code>new</code>/<code>delete</code> in C++).</li><li>Risk of <strong>memory leaks</strong> (unused memory not deallocated) and <strong>dangling pointers</strong> (pointers to deallocated memory).</li></ul><h3>Garbage Collection:</h3><ul><li>The runtime environment (JVM, .NET CLR, Python interpreter, etc.) tracks memory allocation and usage.</li><li>Periodically identifies 'garbage' (memory objects no longer referenced by the running program).</li><li>Reclaims this memory, making it available for future allocations.</li></ul><h3>Common Algorithms:</h3><ul><li><strong>Reference Counting:</strong> Keep a count of references to each object. When count reaches zero, object is garbage. Simple, but cannot handle reference cycles.</li><li><strong>Mark-and-Sweep:</strong> Two phases: Mark reachable objects starting from root references, then Sweep (deallocate) all unmarked objects. Handles cycles.</li><li><strong>Copying Collectors:</strong> Divide memory into sections, copy live objects from one section to another during collection. Naturally defragments memory.</li><li><strong>Generational GC:</strong> Based on the observation that most objects die young. Divides heap into 'generations' and collects younger generations more frequently.</li></ul><h3>Takeaway:</h3><p>Garbage Collection simplifies programming by automating memory deallocation, reducing memory errors like leaks and dangling pointers. However, it introduces runtime overhead and potential pauses ('stop-the-world' events) during collection cycles, which can be optimized by different algorithms and tuning.</p>"
    },
    {
        "title": "CS: Big O Notation (Advanced Considerations)",
        "content": "<h3>Concept: Analyzing Algorithm Efficiency (Beyond Basics)</h3><p>Quantifying how the running time or space requirements of an algorithm grow as the input size increases.</p><h3>Basics Refresher:</h3><ul><li>O(1): Constant time (e.g., array access)</li><li>O(log n): Logarithmic time (e.g., binary search)</li><li>O(n): Linear time (e.g., traversing a list)</li><li>O(n log n): Log-linear time (e.g., efficient sorts like MergeSort, QuickSort average)</li><li>O(n^2): Quadratic time (e.g., nested loops, bubble sort)</li><li>O(2^n): Exponential time (e.g., brute-force solutions to some problems)</li></ul><h3>Beyond Asymptotic Growth:</h3><ul><li><strong>Constants Matter (Sometimes):</strong> O(n) is asymptotically better than O(n^2), but for small 'n', an algorithm with a large constant factor might be slower than one with a smaller factor.</li><li><strong>Average vs. Worst Case:</strong> Big O typically refers to the worst case, but average case (e.g., QuickSort O(n log n) average vs O(n^2) worst) is also important.</li><li><strong>Space Complexity:</strong> Big O applies to memory usage too. O(1) space is ideal.</li><li><strong>Amortized Analysis:</strong> Average performance of an operation over a sequence of operations, e.g., O(1) amortized time for dynamic array append despite occasional O(n) resizes.</li><li><strong>Little O (o):</strong> Strict inequality. f(n) is o(g(n)) if lim(f(n)/g(n)) as n->inf = 0. Means f grows *strictly slower* than g.</li><li><strong>Theta (Θ):</strong> Tight bound. f(n) is Θ(g(n)) if it's both O(g(n)) and Ω(g(n)). Means f grows at the *same rate* as g.</li><li><strong>Omega (Ω):</strong> Lower bound. f(n) is Ω(g(n)) if f grows *at least as fast* as g.</li></ul><h3>Takeaway:</h3><p>Big O (O) provides an upper bound on growth. For more precise analysis, consider Ω (lower bound) and Θ (tight bound). Understand average vs. worst case, and remember that constants and specific input distributions can be relevant in practice, even if they don't change the Big O class.</p>"
    },
    {
        "title": "CS: P vs. NP",
        "content": "<h3>Concept: Problem Complexity Classes</h3><p>A fundamental open question in theoretical computer science concerning the relationship between two classes of computational problems.</p><h3>Complexity Class P:</h3><ul><li>Contains decision problems (problems with a 'yes' or 'no' answer).</li><li>Problems in P are those that can be solved by a <strong>deterministic Turing machine in polynomial time</strong> (i.e., O(n^k) for some constant k, where n is the input size).</li><li>These problems are considered 'tractable' or 'easy' to solve algorithmically.</li><li>Examples: Sorting, searching, finding shortest path in unweighted graph.</li></ul><h3>Complexity Class NP:</h3><ul><li>Contains decision problems.</li><li>Problems in NP are those for which a potential solution can be <strong>verified in polynomial time</strong> by a deterministic Turing machine. The 'N' stands for 'Non-deterministic' (referring to a non-deterministic Turing machine which can solve them in polynomial time).</li><li>Finding a solution might be hard, but checking if a proposed solution is correct is easy.</li><li>Examples: Satisfiability (SAT), Traveling Salesperson Problem (decision version), Subset Sum.</li></ul><h3>The P vs. NP Question:</h3><p>Is P = NP? Does every problem whose solution can be *verified* quickly also have a solution that can be *found* quickly? Most computer scientists believe P ≠ NP.</p><h3>NP-Complete:</h3><p>A subset of NP problems that are the 'hardest' in NP. Any problem in NP can be reduced to an NP-Complete problem in polynomial time. If an NP-Complete problem could be solved in polynomial time, then P would equal NP.</p><h3>Takeaway:</h3><p>P problems are those solvable in polynomial time. NP problems are those whose solutions are verifiable in polynomial time. The P=NP question asks if finding is as easy as verifying. NP-Complete problems are central to this debate, being the hardest problems in NP. Proving P=NP or P≠NP would be a monumental breakthrough.</p>"
    },
    {
        "title": "CS: File System Basics",
        "content": "<h3>Concept: Organizing Data on Storage</h3><p>The method and data structures that an operating system uses to control how data is stored and retrieved on a storage device (like a hard drive, SSD, USB drive).</p><h3>Key Functions:</h3><ul><li><strong>Organizing Data:</strong> Provides a hierarchical structure (directories/folders) to group files.</li><li><strong>Managing Space:</strong> Tracks used and free space on the device. Allocates space for files.</li><li><strong>Providing Metadata:</strong> Stores information about files (name, size, creation/modification date, permissions, owner).</li><li><strong>Ensuring Data Integrity:</strong> Protecting data from corruption (though reliability varies by filesystem).</li><li><strong>Access Control:</strong> Implementing permissions to restrict who can read/write/execute files.</li></ul><h3>Core Components (Conceptual):</h3><ul><li><strong>Boot Block:</strong> Contains bootstrapping code to load the OS.</li><li><strong>Superblock:</strong> Contains metadata about the entire file system (size, block size, free block list, etc.).</li><li><strong>Inode/File Entry Structure:</strong> Stores metadata for each file/directory, including pointers to its data blocks. (Often `inode` in Unix-like, `file entry` in others).</li><li><strong>Data Blocks:</strong> The actual content of files is stored in fixed-size blocks on the device.</li><li><strong>Directory Structure:</strong> Stores names and pointers (like inode numbers) to the files and subdirectories within it.</li></ul><h3>Takeaway:</h3><p>File systems are essential for persistent data storage, providing structure, management, and access control on storage devices. Different file systems (e.g., ext4, NTFS, APFS) implement these concepts with varying structures, performance characteristics, and feature sets.</p>"
    },
    {
        "title": "CS: Object-Oriented vs. Functional Programming",
        "content": "<h3>Concept: Programming Paradigms</h3><p>Different styles or ways of thinking about structuring programs.</p><h3>Object-Oriented Programming (OOP):</h3><ul><li><strong>Focus:</strong> Data and the methods that operate on that data are bundled together into <strong>objects</strong>.</li><li><strong>Core Concepts:</strong> Encapsulation (bundling data/methods), Inheritance (creating new classes from existing ones), Polymorphism (objects of different classes responding to the same method call).</li><li><strong>State:</strong> Objects often have internal state that can change.</li><li><strong>Approach:</strong> Model the world as interacting objects. Imperative style is common (changing state).</li><li><strong>Examples:</strong> Java, C++, Python, C# (support OOP).</li></ul><h3>Functional Programming (FP):</h3><ul><li><strong>Focus:</strong> Computation is treated as the evaluation of <strong>mathematical functions</strong>.</li><li><strong>Core Concepts:</strong> First-class functions (functions as data), Immutability (data is generally not changed after creation), Pure functions (always return same output for same input, no side effects), Higher-order functions (functions that take/return other functions).</li><li><strong>State:</strong> Minimize mutable state; prefer creating new data from old.</li><li><strong>Approach:</strong> Define what needs to be computed using expressions and function composition. Declarative style is common.</li><li><strong>Examples:</strong> Haskell (pure FP), Lisp, Clojure, Scala, F#, JavaScript (supports FP).</li></ul><h3>Takeaway:</h3><p>OOP organizes code around data and objects with mutable state, emphasizing encapsulation and inheritance. FP organizes code around functions and immutable data, emphasizing pure functions and composition. Many modern languages support elements of both paradigms, allowing developers to choose the best approach for different parts of a system.</p>"
    },
    {
        "title": "Math: Linear Independence (Linear Algebra)",
        "content": "<h3>Concept: Non-Redundancy in Vectors</h3><p>A set of vectors <code>{v1, v2, ..., vk}</code> is <strong>linearly independent</strong> if the only way to form a linear combination of these vectors that results in the zero vector is when all the scalar coefficients are zero. It signifies that no vector in the set can be expressed as a combination of the others.</p><h3>Defining Equation & Trivial Solution:</h3><p>Consider the linear combination:<br><code>c1*v1 + c2*v2 + ... + ck*vk = 0</code> (where <code>0</code> is the zero vector)</p><p>This set of vectors is linearly independent <em>if and only if</em> the <em>only possible values</em> for the scalar coefficients (<code>c1, c2, ..., ck</code>) that satisfy this equation are:<br><code>c1 = 0, c2 = 0, ..., ck = 0</code></p><p>This unique solution where all coefficients are zero is called the <strong>trivial solution</strong>.</p><h3>Intuition & Geometric Interpretation:</h3><p>Think of the vectors as directions. If a set of vectors is linearly independent, adding any multiple of one vector will move you in a direction that *cannot* be achieved by only combining multiples of the other vectors. Each vector contributes uniquely to the set's span.</p><ul><li>In 2D: Two vectors are linearly independent if they are not parallel (one is not a scalar multiple of the other). They 'point' in different directions, allowing you to reach any point in the 2D plane (if they are non-zero).</li><li>In 3D: Three vectors are linearly independent if they don't all lie in the same plane passing through the origin.</li></ul><p>Each linearly independent vector adds a new dimension to the space they span.</p><h3>Contrast (Linear Dependence):</h3><p>If there exists *any* solution to the equation <code>c1*v1 + ... + ck*vk = 0</code> where <em>at least one</em> coefficient <code>ci</code> is <em>not</em> zero (a <strong>non-trivial solution</strong>), the set is <strong>linearly dependent</strong>.</p><p>If a set is linearly dependent, it means at least one vector <code>vi</code> in the set *can* be written as a linear combination of the *other* vectors. For example, if <code>c1*v1 + c2*v2 = 0</code> with <code>c1 ≠ 0</code>, then <code>v1 = (-c2/c1)*v2</code>, meaning <code>v1</code> is a scalar multiple of <code>v2</code> (they are collinear). This indicates redundancy in the set.</p><h3>Checking for Linear Independence:</h3><p>To check if a set of vectors is linearly independent, you can form a matrix where the columns (or rows) are the vectors. Then, solve the system of linear equations represented by <code>Ac = 0</code> (where A is the matrix and c is the vector of coefficients).</p><ul><li>The set is linearly independent if and only if the matrix A has <strong>full column rank</strong> (or its determinant is non-zero if it's a square matrix), which implies the system <code>Ac = 0</code> has only the trivial solution.</li><li>Alternatively, use Gaussian elimination to row reduce the matrix. The columns corresponding to pivot positions are linearly independent. If the number of pivot columns equals the number of vectors, they are linearly independent.</li></ul><h3>Takeaway:</h3><p>Linear independence is a core concept in Linear Algebra. It is fundamental for understanding the <strong>span</strong> of a set of vectors and for defining a <strong>basis</strong> – a minimal set of vectors that can generate an entire vector space. Linearly independent vectors are essential building blocks that are not redundant.</p>"
    },
    {
        "title": "CS: Cache Memory",
        "content": "<h3>Concept: Speeding up Memory Access</h3><p>A small, fast type of memory used by the CPU to store frequently accessed data and instructions, reducing the need to access slower main memory (RAM).</p><h3>Locality of Reference:</h3><p>Caches work based on these principles:</p><ul><li><strong>Temporal Locality:</strong> If an item is referenced, it will likely be referenced again soon.</li><li><strong>Spatial Locality:</strong> If an item is referenced, items whose addresses are close by will likely be referenced soon.</li></ul><h3>Structure & Operation:</h3><ul><li>Organized in multiple levels (L1, L2, L3), with L1 being the smallest and fastest, closest to the CPU core.</li><li>Memory is divided into fixed-size blocks (<strong>cache lines</strong>).</li><li>When data is needed, the CPU checks the cache first (<strong>cache hit</strong>). If found, it's retrieved quickly.</li><li>If not found (<strong>cache miss</strong>), the CPU fetches a block containing the needed data from the next level of cache or main memory, and loads it into the cache.</li><li><strong>Cache coherence</strong> protocols ensure consistency across multiple cores with their own caches.</li></ul><h3>Takeaway:</h3><p>Cache memory is critical for modern CPU performance by exploiting locality of reference to reduce the average time spent waiting for data, effectively bridging the speed gap between the CPU and main memory.</p>"
    },
    {
        "title": "CS: OS Scheduling Algorithms",
        "content": "<h3>Concept: Managing CPU Time</h3><p>Algorithms used by the operating system to decide which process or thread in the ready queue should be executed next by the CPU.</p><h3>Goals:</h3><ul><li>Maximize CPU utilization.</li><li>Maximize Throughput (jobs completed per unit time).</li><li>Minimize Turnaround Time (total time from submission to completion).</li><li>Minimize Waiting Time (time spent in the ready queue).</li><li>Minimize Response Time (time from request submission until the first response is produced).</li><li>Ensure Fairness.</li></ul><h3>Common Algorithms:</h3><ul><li><strong>First-Come, First-Served (FCFS):</strong> Processes executed in the order they arrive. Simple, but can have high average waiting time (Convoy Effect). Non-preemptive.</li><li><strong>Shortest Job Next (SJN/SJF):</strong> Process with the smallest predicted execution time is executed next. Optimal for minimum average waiting time, but requires knowing future times and can suffer from starvation. Can be preemptive or non-preemptive.</li><li><strong>Priority Scheduling:</strong> Each process has a priority; highest priority process is executed first. Can suffer from starvation for low-priority processes (can be mitigated with Aging). Can be preemptive or non-preemptive.</li><li><strong>Round Robin (RR):</strong> Each process gets a small unit of CPU time (<strong>time slice</strong>). If it doesn't finish, it's preempted and moved to the end of the queue. Good for time-sharing systems, provides better response time. Preemptive.</li><li><strong>Multilevel Queue Scheduling:</strong> Ready queue divided into multiple queues (e.g., foreground interactive, background batch) with different scheduling algorithms.</li></ul><h3>Takeaway:</h3><p>OS scheduling algorithms balance competing goals to efficiently allocate CPU time among processes. The choice of algorithm depends on the system's goals (batch vs. interactive, real-time constraints) and involves trade-offs between fairness, throughput, and response times.</p>"
    },
     {
        "title": "CS: Union-Find (Disjoint Set Union)",
        "content": "<h3>Concept: Managing Disjoint Sets</h3><p>A data structure that keeps track of a set of elements partitioned into a number of disjoint (non-overlapping) subsets.</p><h3>Core Operations:</h3><ul><li><strong><code>Find(element)</code>:</strong> Determines which subset a particular element belongs to. Returns a 'representative' of that subset (e.g., the root of a tree).</li><li><strong><code>Union(set1, set2)</code>:</strong> Merges two subsets into a single subset. Typically done by merging the trees of their representatives.</li></ul><h3>Implementation:</h3><p>Usually implemented using a forest of trees. Each element is a node. The parent pointer indicates the set representative (or another node on the path to the representative). The root of each tree is the representative of its set.</p><h3>Optimizations:</h3><ul><li><strong>Union by Rank/Size:</strong> When merging, attach the smaller tree to the root of the larger tree to keep trees relatively flat.</li><li><strong>Path Compression:</strong> During a <code>Find</code> operation, make every node on the path from the element to the root point directly to the root. Speeds up future finds.</li></ul><h3>Performance:</h3><p>With both optimizations, the time complexity for M operations on N elements is nearly constant on average, often cited using the inverse Ackermann function (which grows extremely slowly), effectively O(α(N)) amortized time, close to O(1).</p><h3>Use Cases:</h3><ul><li>Detecting cycles in graphs (e.g., Kruskal's MST algorithm).</li><li>Checking connectivity in graphs.</li><li>Image processing (connected components).</li><li>Network connectivity problems.</li></ul><h3>Takeaway:</h3><p>The Union-Find data structure is highly efficient for managing disjoint sets and performing merge/find operations, making it invaluable in graph algorithms and problems involving connectivity.</p>"
    },
    {
        "title": "CS: Topological Sorting",
        "content": "<h3>Concept: Ordering Tasks with Dependencies</h3><p>A linear ordering of vertices in a directed acyclic graph (DAG) such that for every directed edge <code>u -> v</code>, vertex <code>u</code> comes before vertex <code>v</code> in the ordering.</p><h3>Key Properties:</h3><ul><li>Applicable <strong>only</strong> to Directed Acyclic Graphs (DAGs). A graph with a cycle cannot be topologically sorted.</li><li>There can be multiple valid topological sorts for a given DAG.</li><li>Used to schedule tasks or events based on their dependencies.</li></ul><h3>Algorithms:</h3><ul><li><strong>Kahn's Algorithm (BFS-based):</strong></li>    <ul><li>Find all vertices with an in-degree of 0. Add them to a queue.</li><li>While the queue is not empty:</li>        <ul><li>Dequeue a vertex <code>u</code> and add it to the result list.</li><li>For each neighbor <code>v</code> of <code>u</code>: decrement the in-degree of <code>v</code>. If <code>v</code>'s in-degree becomes 0, enqueue <code>v</code>.</li></ul><li>If the number of vertices in the result list equals the total number of vertices, a topological sort exists. Otherwise, there was a cycle.</li></ul><li><strong>DFS-based Algorithm:</strong></li>    <ul><li>Perform a Depth-First Search on the graph.</li><li>When a vertex finishes its DFS traversal (all its descendants have been visited), push it onto a stack (or prepend it to a list).</li><li>The resulting order in the stack/list is a topological sort.</li><li>Cycles can be detected during DFS (e.g., encountering a node already in the current recursion stack).</li></ul></ul><h3>Takeaway:</h3><p>Topological sorting provides a valid sequence for processing elements that have dependencies. It's essential for task scheduling, course prerequisite ordering, build systems, and anywhere items must be processed in a specific order determined by relationships.</p>"
    },
     {
        "title": "CS: Semaphores vs. Mutexes",
        "content": "<h3>Concept: Synchronization Primitives</h3><p>Tools used in concurrent programming to control access to shared resources and prevent issues like race conditions.</p><h3>Mutex (Mutual Exclusion):</h3><ul><li>A binary lock. It's either locked or unlocked.</li><li>Used to protect a <strong>critical section</strong> of code, ensuring that only one thread/process can access the shared resource at a time.</li><li>Provides <strong>exclusive access</strong>.</li><li>Typically associated with <strong>ownership</strong>: the thread/process that locks the mutex is the only one that can unlock it.</li><li>Often implemented as a simple boolean flag or an atomic variable.</li><li>Common operations: <code>acquire()</code> (wait if locked, then lock), <code>release()</code> (unlock).</li></ul><h3>Semaphore:</h3><ul><li>A signaling mechanism. Has an integer value (count) >= 0.</li><li>Represents a number of available resources.</li><li>Can be <strong>counting</strong> (value > 1) or <strong>binary</strong> (value 0 or 1, similar to a mutex but without ownership).</li><li>Used for both controlling access to multiple resources and for signaling between processes/threads.</li><li>Common operations: <code>wait()</code> (or <code>P()</code>, <code>down()</code>): decrements the count. If count becomes negative, block. <code>signal()</code> (or <code>V()</code>, <code>up()</code>): increments the count. If threads are blocked waiting, unblock one.</li><li>Does <strong>not</strong> have the concept of ownership. Any thread can signal a semaphore.</li></ul><h3>Distinction:</h3><p>While a binary semaphore *can* behave like a mutex, their intent differs. Mutexes are for protecting shared data/critical sections (resource *control*). Semaphores are for signaling between processes/threads or managing a pool of resources (resource *counting/signaling*).</p><h3>Takeaway:</h3><p>Use a Mutex for exclusive access to a single shared resource (like a data structure). Use a Semaphore to control access to a pool of resources (e.g., buffer slots) or for complex signaling coordination between concurrent entities.</p>"
    },
    {
        "title": "CS: Memory Allocation (Heap vs. Stack)",
        "content": "<h3>Concept: Where Data Lives in Memory</h3><p>Two primary regions of memory used by programs to store data, managed differently by the operating system and programming language runtime.</p><h3>Stack:</h3><ul><li>Used for <strong>static memory allocation</strong> and function call management.</li><li>Stores <strong>local variables</strong>, function parameters, and return addresses.</li><li>Memory is managed automatically: allocated when a function is called, deallocated when the function returns (LIFO - Last In, First Out).</li><li>Fast access because allocation/deallocation is simple pointer adjustment.</li><li>Fixed size (per thread/process) or small allocation limit, prone to <strong>Stack Overflow</strong> if recursion is too deep or local variables are too large.</li></ul><h3>Heap:</h3><ul><li>Used for <strong>dynamic memory allocation</strong>.</li><li>Stores data whose size is not known at compile time or whose lifetime needs to extend beyond the function call where it was created.</li><li>Memory must be explicitly allocated and deallocated by the programmer (e.g., <code>malloc</code>/<code>free</code>, <code>new</code>/<code>delete</code>) or managed automatically by a Garbage Collector.</li><li>Slower access than the stack due to complex allocation/deallocation algorithms.</li><li>Larger pool of memory (limited by system resources).</li><li>Prone to <strong>memory leaks</strong> (if not freed/collected) and <strong>fragmentation</strong> (memory becomes broken into small, unusable chunks).</li></ul><h3>Takeaway:</h3><p>The Stack is for short-lived, local data and function calls (automatic management, fast). The Heap is for long-lived or dynamically sized data (manual or garbage-collected management, slower, more complex memory issues).</p>"
    },
    {
        "title": "CS: Caching Strategies (Web/Distributed Systems)",
        "content": "<h3>Concept: Storing Data Closer to the User/Requester</h3><p>Techniques used to store copies of data in a temporary location (a cache) to serve future requests faster than fetching from the original source.</p><h3>Where Caching Happens:</h3><ul><li><strong>Browser Cache:</strong> Stores web page resources (images, CSS, JS) on the user's machine.</li><li><strong>Proxy Cache:</strong> Shared cache for multiple users (e.g., ISP level).</li><li><strong>Application Cache:</strong> Within an application's memory or local storage.</li><li><strong>Database Cache:</strong> Caches query results or frequently accessed data blocks.</li><li><strong>Distributed Cache:</strong> A separate layer of caching servers (e.g., Redis, Memcached) used in distributed systems.</li><li><strong>CDN (Content Delivery Network):</strong> Geographically distributed servers that cache static content (images, videos, files) near end-users.</li></ul><h3>Key Challenges:</h3><ul><li><strong>Cache Invalidation:</strong> Ensuring the data in the cache is up-to-date with the source. Strategies include Time-to-Live (TTL), explicit invalidation, or write-through/write-back policies.</li><li><strong>Cache Coherence:</strong> Ensuring consistency across multiple caches of the same data.</li><li><strong>Cache Misses:</strong> Handling requests for data not present in the cache (results in a slower fetch from the source).</li></ul><h3>Takeaway:</h3><p>Caching is fundamental to improving performance, reducing latency, and decreasing load on origin servers in web and distributed systems. Effective caching requires a strategy for placing data strategically and managing cache invalidation.</p>"
    },
    {
        "title": "CS: Load Balancing",
        "content": "<h3>Concept: Distributing Workload</h3><p>The process of distributing network traffic or processing tasks across a number of servers or computing resources to ensure optimal resource utilization, maximize throughput, minimize response time, and prevent overload of any single resource.</p><h3>Goals:</h3><ul><li>Improve performance (faster responses).</li><li>Increase availability and reliability (if a server fails, traffic is routed elsewhere).</li><li>Improve scalability (easily add or remove resources).</li><li>Reduce cost (utilize resources efficiently).</li></ul><h3>Load Balancing Algorithms:</h3><ul><li><strong>Round Robin:</strong> Distributes requests sequentially to each server in turn. Simple.</li><li><strong>Least Connections:</strong> Directs traffic to the server with the fewest active connections. Good for uneven request processing times.</li><li><strong>Least Response Time:</strong> Directs traffic to the server with the fewest connections and the lowest average response time. Takes server health into account.</li><li><strong>IP Hash:</strong> Uses the client's IP address to determine the server, ensuring requests from the same client go to the same server (useful for stateful applications, though can be uneven).</li><li><strong>Weighted Algorithms:</strong> Distributes traffic based on pre-configured weights assigned to servers (e.g., stronger servers get more requests).</li></ul><h3>Types:</h3><ul><li><strong>Network Load Balancing:</strong> Operates at the transport level (TCP/UDP).</li><li><strong>Application Load Balancing:</strong> Operates at the application level (HTTP/HTTPS), can inspect headers and traffic content.</li></ul><h3>Takeaway:</h3><p>Load balancing is essential in distributed systems and web services for handling high traffic, ensuring high availability, and scaling applications horizontally. The choice of algorithm depends on the application's needs (stateful vs. stateless) and the desired distribution strategy.</p>"
    },
    {
        "title": "CS: CAP Theorem (Distributed Systems)",
        "content": "<h3>Concept: Trade-offs in Distributed Databases</h3><p>A fundamental theorem stating that a distributed data store cannot simultaneously provide more than two out of the following three guarantees:</p><ul><li><strong>Consistency (C):</strong> Every read receives the most recent write or an error. All nodes see the same data at the same time.</li><li><strong>Availability (A):</strong> Every request receives a non-error response, without guaranteeing that the response contains the most recent write. The system is always responsive.</li><li><strong>Partition Tolerance (P):</strong> The system continues to operate despite arbitrary network partitions (communication failures between nodes). Network partitions are a given in real-world distributed systems.</li></ul><h3>The Trade-off:</h3><p>Since network partitions (P) are unavoidable in a truly distributed system, you must choose between Consistency (C) and Availability (A) during a partition event.</p><ul><li><strong>CP System:</strong> Prioritizes Consistency over Availability during a partition. Nodes on the 'wrong' side of a partition might become unavailable to prevent serving stale data. (e.g., traditional RDBMS clusters, ZooKeeper).</li><li><strong>AP System:</strong> Prioritizes Availability over Consistency during a partition. Nodes might serve potentially stale data, aiming for eventual consistency after the partition heals. (e.g., Cassandra, CouchDB, many NoSQL databases).</li></ul><h3>Takeaway:</h3><p>The CAP theorem highlights the inherent limitations of distributed systems. When designing or choosing a distributed database, you must understand its trade-offs regarding Consistency and Availability during network partitions based on your application's requirements.</p>"
    },
    {
        "title": "CS: SQL vs. NoSQL Databases",
        "content": "<h3>Concept: Database Models</h3><p>Comparing traditional relational databases with newer non-relational approaches.</p><h3>SQL Databases (Relational):</h3><ul><li><strong>Structure:</strong> Data is stored in tables with fixed schemas (rows and columns). Relationships are defined via foreign keys.</li><li><strong>Query Language:</strong> Use Structured Query Language (SQL) for defining, manipulating, and querying data.</li><li><strong>Schema:</strong> Generally requires a predefined schema. Schema changes can be complex.</li><li><strong>Scalability:</strong> Traditionally scales vertically (bigger server). Horizontal scaling (sharding) is possible but can be complex.</li><li><strong>ACID Properties:</strong> Typically enforces Atomicity, Consistency, Isolation, Durability for transactions.</li><li><strong>Use Cases:</strong> Applications requiring complex queries, strong transaction guarantees, and structured data (e.g., financial systems, inventory).</li><li><strong>Examples:</strong> MySQL, PostgreSQL, Oracle, SQL Server.</li></ul><h3>NoSQL Databases (Non-Relational):</h3><ul><li><strong>Structure:</strong> Diverse models (document, key-value, wide-column, graph). Schemas are often dynamic or flexible.</li><li><strong>Query Language:</strong> Varies greatly (APIs, declarative query languages).</li><li><strong>Schema:</strong> Dynamic schema allows easier evolution.</li><li><strong>Scalability:</strong> Designed for horizontal scaling (adding more servers) often automatically.</li><li><strong>ACID Properties:</strong> Often relax ACID for better Availability and Partition Tolerance (following BASE principles: Basically Available, Soft state, Eventually consistent).</li><li><strong>Use Cases:</strong> Large volumes of unstructured/semi-structured data, high velocity data, flexible schemas, massive scale (e.g., social media feeds, IoT data, content management).</li><li><strong>Examples:</strong> MongoDB (Document), Cassandra (Wide-Column), Redis (Key-Value), Neo4j (Graph).</li></ul><h3>Takeaway:</h3><p>SQL databases are best for structured data and strong transactional needs. NoSQL databases offer flexibility, scalability, and performance for unstructured data and high-volume, distributed applications, often trading some consistency guarantees.</p>"
    },
    {
        "title": "CS: Database Normalization",
        "content": "<h3>Concept: Organizing Relational Database Schema</h3><p>A process of structuring a relational database by decomposing tables to reduce redundancy and improve data integrity.</p><h3>Goals:</h3><ul><li>Eliminate redundant data storage.</li><li>Ensure data dependencies make sense (data is stored in the correct table).</li><li>Reduce anomalies (Insertion, Update, Deletion anomalies).</li></ul><h3>Anomalies Explained:</h3><ul><li><strong>Insertion Anomaly:</strong> Cannot add certain data unless other unrelated data is also added.</li><li><strong>Update Anomaly:</strong> Updating one piece of data requires updating multiple rows, risking inconsistency if one update fails.</li><li><strong>Deletion Anomaly:</strong> Deleting one piece of data accidentally deletes other unrelated data.</li></ul><h3>Normal Forms (NF):</h3><ul><li><strong>1NF (First Normal Form):</strong> Atomic values (no repeating groups or arrays within a single cell).</li><li><strong>2NF (Second Normal Form):</strong> Is in 1NF and all non-key attributes are fully functionally dependent on the primary key. (Removes partial dependencies).</li><li><strong>3NF (Third Normal Form):</strong> Is in 2NF and all non-key attributes are not transitively dependent on the primary key. (Removes transitive dependencies - where a non-key attribute depends on another non-key attribute).</li><li>Higher forms (BCNF, 4NF, 5NF, etc.) address more complex dependencies.</li></ul><h3>Trade-offs:</h3><p>Normalization reduces redundancy and improves integrity but can lead to more complex queries requiring multiple joins, potentially impacting read performance. Denormalization (intentionally violating normal forms) might be used for performance optimization in data warehousing or specific read-heavy scenarios.</p><h3>Takeaway:</h3><p>Normalization is crucial for designing robust relational databases by minimizing redundancy and preventing anomalies, improving data integrity. The level of normalization (typically 3NF) balances data integrity with query performance needs.</p>"
    },
    {
        "title": "CS: Message Queues",
        "content": "<h3>Concept: Asynchronous Communication</h3><p>A software component used for communication between processes or applications, enabling them to exchange messages asynchronously.</p><h3>How it Works:</h3><ul><li>A sending application (<strong>Producer</strong>) creates a message and sends it to a queue.</li><li>A receiving application (<strong>Consumer</strong>) connects to the queue and retrieves messages for processing.</li><li>The queue acts as a buffer, storing messages until consumers are ready.</li></ul><h3>Benefits:</h3><ul><li><strong>Decoupling:</strong> Producer and consumer don't need to be running or even aware of each other simultaneously. They only need to know the queue.</li><li><strong>Asynchronicity:</strong> Producers can send messages and continue processing without waiting for the consumer to receive or process them.</li><li><strong>Buffering:</strong> Handles spikes in traffic by queuing requests, preventing consumers from being overwhelmed.</li><li><strong>Reliability:</strong> Messages can be persisted, ensuring delivery even if producers or consumers crash.</li><li><strong>Scalability:</strong> Easily add more consumers to process messages in parallel.</li></ul><h3>Use Cases:</h3><ul><li>Task queues (processing background jobs).</li><li>Event-driven architectures.</li><li>Inter-service communication in microservices.</li><li>Decoupling heavy workloads (e.g., image processing, email sending).</li></ul><h3>Examples:</h3><p>RabbitMQ, Apache Kafka, ActiveMQ, Amazon SQS, Azure Service Bus.</p><h3>Takeaway:</h3><p>Message queues are fundamental building blocks for building scalable, reliable, and decoupled distributed systems by enabling asynchronous, buffered communication between disparate components.</p>"
    },
    {
        "title": "CS: RESTful Architecture",
        "content": "<h3>Concept: Architectural Style for APIs</h3><p>Representational State Transfer (REST) is an architectural style for designing networked applications, particularly APIs (Application Programming Interfaces), emphasizing scalability, reliability, and performance.</p><h3>Key Constraints (Principles):</h3><ul><li><strong>Client-Server:</strong> Separation of concerns. Client handles UI, Server handles data/logic.</li><li><strong>Stateless:</strong> Each request from client to server must contain all information needed to understand and complete the request. Server stores no client context between requests.</li><li><strong>Cacheable:</strong> Responses must implicitly or explicitly define themselves as cacheable or not to improve client performance and scalability.</li><li><strong>Layered System:</strong> Client cannot tell if it's connected directly to the end server or to an intermediary (load balancer, proxy) - layers enhance scalability and security.</li><li><strong>Code on Demand (Optional):</strong> Servers can temporarily extend client functionality by sending executable code (e.g., JavaScript).</li><li><strong>Uniform Interface (Most Important):</strong> Simplifies the overall system architecture. Uses standard methods (HTTP verbs) and resource identification.</li></ul><h3>Uniform Interface Details:</h3><ul><li><strong>Identification of Resources:</strong> Resources (data/objects) are identified by URIs (e.g., <code>/users/123</code>).</li><li><strong>Manipulation of Resources through Representations:</strong> Clients interact with resources using representations (e.g., JSON, XML) exchanged in the body of the request/response.</li><li><strong>Self-descriptive Messages:</strong> Each message contains enough information to describe how to process the message (e.g., using media types, link relations).</li><li><strong>Hypermedia as the Engine of Application State (HATEOAS):</strong> Clients interact with the application state by following links provided in the server's responses. (Often the least strictly followed constraint in practice).</li></ul><h3>Takeaway:</h3><p>REST is an architectural style that leverages standard web technologies (HTTP, URIs) to build scalable and maintainable APIs by adhering to principles like statelessness, client-server separation, and a uniform interface. While often used loosely, true REST adheres to all constraints, including HATEOAS.</p>"
    },
    {
        "title": "CS: Idempotence (API Design)",
        "content": "<h3>Concept: Safe Retries</h3><p>In the context of APIs and distributed systems, an operation is <strong>idempotent</strong> if performing it multiple times with the same parameters has the same effect as performing it exactly once.</p><h3>Intuition:</h3><p>You can safely retry an idempotent request without causing unintended side effects if the first attempt failed or timed out *after* the action was successfully performed on the server side.</p><h3>Examples:</h3><ul><li><strong>Idempotent:</strong></li>    <ul><li>Reading data (<code>GET</code> request). Reading multiple times doesn't change the data.</li><li>Deleting a resource (<code>DELETE</code> request). Deleting a resource that's already deleted has no further effect on the resource's state (though the *response* might differ - 200 vs 404).</li><li>Updating a resource to a specific state (e.g., <code>PUT /items/123 {state: 'processed'}</code>). Setting the state to 'processed' multiple times results in the state being 'processed'.</li></ul><li><strong>Non-Idempotent:</strong></li>    <ul><li>Creating a resource via a POST request (<code>POST /orders {item: 'book'}</code>). Sending this multiple times would create multiple orders.</li><li>Incrementing a counter (e.g., <code>POST /counter/increment</code>). Sending this multiple times increments the counter multiple times.</li></ul></ul><h3>Why it Matters:</h3><p>Network issues, server timeouts, or client retries are common in distributed systems. Idempotent operations simplify error handling and retry logic, preventing duplicate actions or data inconsistencies.</p><h3>Implementing Idempotence:</h3><p>For non-idempotent operations (like creating resources), idempotence can often be *achieved* by clients providing a unique idempotency key (often a UUID) with the request. The server stores this key and the result of the first successful request. Subsequent requests with the same key within a certain time window return the stored result without re-executing the core logic.</p><h3>Takeaway:</h3><p>Designing APIs with idempotent operations (or implementing idempotency mechanisms for non-idempotent ones) is crucial for building robust and reliable distributed systems that can tolerate network failures and client retries gracefully.</p>"
    },
    {
        "title": "CS: Bloom Filters",
        "content": "<h3>Concept: Probabilistic Membership Testing</h3><p>A space-efficient probabilistic data structure used to test whether an element is a member of a set. Can return false positives, but *never* false negatives.</p><h3>Structure:</h3><ul><li>Consists of a bit array of fixed size (m).</li><li>Uses multiple independent hash functions (k).</li></ul><h3>Operation:</h3><ul><li><strong>Add:</strong> To add an element, feed it to each of the k hash functions. Each hash function outputs an index in the bit array. Set the bits at all k indices to 1.</li><li><strong>Check:</strong> To check if an element is in the set, feed it to each of the k hash functions. If all k bits at the resulting indices in the array are 1, the filter *might* contain the element (<strong>possible positive</strong>). If *any* of the bits is 0, the element is *definitely not* in the set (<strong>true negative</strong>).</li></ul><h3>False Positives:</h3><p>A false positive occurs when you check for an element that is NOT in the set, but due to hash collisions, all the corresponding bits happen to be set to 1 by other elements. The probability of a false positive increases with the number of elements added and decreases with the size of the bit array (m) and the number of hash functions (k).</p><h3>Trade-offs:</h3><p>Space efficiency vs. the probability of false positives. Increasing the size of the bit array or the number of hash functions reduces false positives but increases memory usage.</p><h3>Use Cases:</h3><ul><li>Checking if a username is already taken (reduces database lookups).</li><li>Web browser URLs already visited.</li><li>Database query filtering.</li><li>Network packet filtering.</li><li>Preventing caching of non-existent objects.</li></ul><h3>Takeaway:</h3><p>Bloom filters are useful when memory is constrained and some false positives are acceptable. They provide a quick 'might be in set' or a definite 'not in set' answer, often significantly reducing the need for more expensive lookups in a primary data store.</p>"
    },
     {
        "title": "CS: Finite State Machines (FSM / FSA)",
        "content": "<h3>Concept: Modeling States and Transitions</h3><p>A mathematical model of computation that consists of a set of states, a set of input symbols, and a transition function that maps a state and an input symbol to a next state.</p><h3>Components:</h3><ul><li><strong>States:</strong> A finite set of possible conditions the machine can be in.</li><li><strong>Input Alphabet:</strong> A finite set of possible input symbols.</li><li><strong>Transition Function:</strong> Determines the next state based on the current state and the input symbol.</li><li><strong>Start State:</strong> The initial state when the machine begins.</li><li><strong>Accept/Final States (Optional):</strong> A subset of states indicating successful processing (used in acceptors/recognizers).</li></ul><h3>Types:</h3><ul><li><strong>Deterministic Finite Automaton (DFA):</strong> For each state and input symbol, there is exactly one next state.</li><li><strong>Nondeterministic Finite Automaton (NFA):</strong> For a state and input symbol, there can be zero, one, or more next states, including transitions on an empty input (epsilon transitions). Any NFA can be converted to an equivalent DFA.</li></ul><h3>Use Cases:</h3><ul><li>Designing lexical analyzers (scanners) in compilers.</li><li>Pattern matching (e.g., regular expressions).</li><li>Modeling system behavior (user interfaces, protocols).</li><li>Control systems.</li><li>Game AI.</li></ul><h3>Takeaway:</h3><p>Finite State Machines are a simple yet powerful model for representing systems with a finite number of states and well-defined transitions triggered by inputs. They are fundamental in theoretical computer science and widely applied in parsing, control, and modeling.</p>"
    }
]
